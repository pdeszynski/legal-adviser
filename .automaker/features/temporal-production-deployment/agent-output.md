I'll start by exploring the codebase to understand the existing structure and then plan the implementation for a production-grade Temporal deployment.

ðŸ”§ Tool: Task
Input: {
  "description": "Explore Temporal module structure",
  "prompt": "Explore the codebase to understand the existing Temporal module structure. Look for:\n\n1. The temporal module directory at `apps/backend/src/modules/temporal/`\n2. Any existing Docker Compose files for Temporal\n3. Current Temporal configuration files\n4. How the backend connects to Temporal\n5. Any existing worker implementations\n\nProvide a comprehensive summary of:\n- Current Temporal setup (development vs production)\n- Existing workflows and activities\n- How workers are configured\n- Current deployment configuration (docker-compose, etc.)\n- What needs to be added for production deployment",
  "subagent_type": "Explore"
}

ðŸ”§ Tool: Glob
Input: {
  "pattern": "**/temporal/**"
}

ðŸ”§ Tool: LS
Input: {
  "path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "ls -la /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal",
  "description": "List temporal module directory contents"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "ls -la /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal | grep -E \"(docker-compose|temporal)\" | head -20",
  "description": "Check for Docker Compose and Temporal files"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/docker-compose.yml"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/docker-compose.infra.yml"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/temporal.module.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/temporal.service.ts",
  "limit": 100
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/temporal.worker.ts",
  "limit": 100
}

ðŸ”§ Tool: Bash
Input: {
  "command": "find /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/workflows -name \"*.workflow.ts\" | head -20",
  "description": "Find workflow files"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "find /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/activities -name \"*.activities.ts\" | head -20",
  "description": "Find activity files"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/temporal.constants.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/workflows/document/document-generation.workflow.ts",
  "limit": 100
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/workflows/notification/email-sending.starter.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/bull-to-temporal-email-migration.ts",
  "limit": 100
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/docker-compose.override.yml"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/temporal-config.yml"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/app.module.ts",
  "limit": 100
}

ðŸ”§ Tool: Bash
Input: {
  "command": "grep -n \"TemporalModule\" /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/app.module.ts",
  "description": "Find TemporalModule usage in app.module.ts"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "find /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules -name \"workflow-starter.ts\" -o -name \"starter.ts\" | head -10",
  "description": "Find workflow starter files"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "find /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/workflows -name \"starter.ts\" | head -10",
  "description": "Find workflow starter files in temporal module"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "ls -la /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/workflows/ | grep starter",
  "description": "List starter files in workflows directory"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "find /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/workflows/ -name \"*.starter.ts\"",
  "description": "Find all starter files with .starter.ts extension"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "grep -r \"ruling-indexing\" /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/workflows/ | grep -v \".starter.ts\" | head -5",
  "description": "Check for ruling-indexing workflow usage"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "grep -r \"subscription-renewal\" /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/ | grep -v \".starter.ts\" | head -5",
  "description": "Check for subscription-renewal workflow usage"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "grep -r \"invoice-generation\" /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/ | grep -v \".starter.ts\" | head -5",
  "description": "Check for invoice-generation workflow usage"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "find /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/activities -name \"*.activities.ts\"",
  "description": "List all activity files"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "grep -A 10 -B 2 \"temporal\" /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/package.json",
  "description": "Check Temporal dependencies in package.json"
}
Now I have a good understanding of the existing Temporal setup. Let me read the current configuration files to understand the specifics better.

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/docker-compose.yml"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/docker-compose.override.yml"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/temporal-config.yml"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/temporal.module.ts"
}
Now let me read the existing temporal worker service and check for any Kubernetes manifests:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/temporal.worker.ts"
}

ðŸ”§ Tool: Glob
Input: {
  "pattern": "**/*.yaml",
  "path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal"
}

ðŸ”§ Tool: Glob
Input: {
  "pattern": "**/k8s/**/*",
  "path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal"
}
Now I have a good understanding of the codebase. Let me create a plan for implementing production-grade Temporal deployment. I'll use the EnterPlanMode tool to plan this implementation.

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "content": "Explore and understand existing Temporal setup",
      "status": "completed",
      "activeForm": "Exploring existing Temporal setup"
    },
    {
      "content": "Create production Docker Compose configuration for Temporal cluster",
      "status": "pending",
      "activeForm": "Creating production Docker Compose configuration"
    },
    {
      "content": "Create Kubernetes manifests for Temporal deployment",
      "status": "pending",
      "activeForm": "Creating Kubernetes manifests"
    },
    {
      "content": "Set up TLS/SSL configuration for Temporal services",
      "status": "pending",
      "activeForm": "Setting up TLS/SSL configuration"
    },
    {
      "content": "Configure Prometheus metrics for Temporal",
      "status": "pending",
      "activeForm": "Configuring Prometheus metrics"
    },
    {
      "content": "Set up Temporal UI with authentication",
      "status": "pending",
      "activeForm": "Setting up Temporal UI with authentication"
    },
    {
      "content": "Implement health checks and monitoring",
      "status": "pending",
      "activeForm": "Implementing health checks and monitoring"
    },
    {
      "content": "Configure archival for workflow history",
      "status": "pending",
      "activeForm": "Configuring archival for workflow history"
    },
    {
      "content": "Create deployment documentation",
      "status": "pending",
      "activeForm": "Creating deployment documentation"
    }
  ]
}
Let me start implementing the production Temporal deployment. First, I'll create the production Docker Compose configuration for a self-hosted Temporal cluster.

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Exploring existing Temporal setup",
      "content": "Explore and understand existing Temporal setup",
      "status": "completed"
    },
    {
      "activeForm": "Creating production Docker Compose configuration",
      "content": "Create production Docker Compose configuration for Temporal cluster",
      "status": "in_progress"
    },
    {
      "activeForm": "Creating Kubernetes manifests",
      "content": "Create Kubernetes manifests for Temporal deployment",
      "status": "pending"
    },
    {
      "activeForm": "Setting up TLS/SSL configuration",
      "content": "Set up TLS/SSL configuration for Temporal services",
      "status": "pending"
    },
    {
      "activeForm": "Configuring Prometheus metrics",
      "content": "Configure Prometheus metrics for Temporal",
      "status": "pending"
    },
    {
      "activeForm": "Setting up Temporal UI with authentication",
      "content": "Set up Temporal UI with authentication",
      "status": "pending"
    },
    {
      "activeForm": "Implementing health checks and monitoring",
      "content": "Implement health checks and monitoring",
      "status": "pending"
    },
    {
      "activeForm": "Configuring archival for workflow history",
      "content": "Configure archival for workflow history",
      "status": "pending"
    },
    {
      "activeForm": "Creating deployment documentation",
      "content": "Create deployment documentation",
      "status": "pending"
    }
  ]
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/docker-compose.temporal.yml",
  "content": "# ============================================================================\n# Docker Compose - Production Temporal Cluster\n# ============================================================================\n#\n# This file contains a production-grade Temporal cluster setup with:\n# - Frontend service (replicated for HA)\n# - History service (replicated for HA)\n# - Matching service (replicated for HA)\n# - Worker service (replicated)\n# - PostgreSQL for backend persistence\n# - Prometheus for metrics\n# - Grafana for metrics visualization\n# - Temporal UI with authentication\n#\n# This is a self-hosted deployment suitable for production use.\n# For Kubernetes deployment, see k8s/temporal/ directory.\n#\n# Usage:\n#   docker compose -f docker-compose.temporal.yml up -d\n#\n# Environment variables:\n#   Create a .env file with production values for:\n#   - POSTGRES_PASSWORD, TEMPORAL_TLS_CERT, etc.\n#\n# ============================================================================\n\nservices:\n  # --------------------------------------------------------------------------\n  # PostgreSQL - Primary Database for Temporal\n  # --------------------------------------------------------------------------\n  postgres-temporal:\n    image: postgres:15-alpine\n    container_name: temporal-postgres\n    environment:\n      POSTGRES_USER: ${TEMPORAL_POSTGRES_USER:-temporal}\n      POSTGRES_PASSWORD: ${TEMPORAL_POSTGRES_PASSWORD:-change-me-in-production}\n      POSTGRES_DB: ${TEMPORAL_POSTGRES_DB:-temporal}\n    ports:\n      - '${TEMPORAL_POSTGRES_PORT:-5433}:5432'\n    volumes:\n      - temporal_postgres_data:/var/lib/postgresql/data\n      - ./apps/backend/src/modules/temporal/schema/temporal-postgres-init.sql:/docker-entrypoint-initdb.d/01-temporal.sql\n    restart: always\n    healthcheck:\n      test: ['CMD-SHELL', 'pg_isready -U ${TEMPORAL_POSTGRES_USER:-temporal} -d ${TEMPORAL_POSTGRES_DB:-temporal}']\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    networks:\n      - temporal-network\n\n  # --------------------------------------------------------------------------\n  # Elasticsearch - For Temporal visibility and search\n  # --------------------------------------------------------------------------\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0\n    container_name: temporal-elasticsearch\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=false\n      - ES_JAVA_OPTS=-Xms1g -Xmx1g\n      - bootstrap.memory_lock=true\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    ports:\n      - '9200:9200'\n    volumes:\n      - temporal_elasticsearch_data:/usr/share/elasticsearch/data\n    healthcheck:\n      test: ['CMD-SHELL', 'curl -f http://localhost:9200/_cluster/health || exit 1']\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 40s\n    networks:\n      - temporal-network\n\n  # --------------------------------------------------------------------------\n  # Temporal Frontend Service\n  # --------------------------------------------------------------------------\n  temporal-frontend:\n    image: temporalio/server:latest\n    container_name: temporal-frontend\n    ports:\n      - '7233:7233'\n    environment:\n      - SERVICES=frontend\n      - TEMPORAL_ADDRESS=${TEMPORAL_ADDRESS:-0.0.0.0:7233}\n      - DB=postgresql\n      - DB_PORT=5432\n      - POSTGRES_USER=${TEMPORAL_POSTGRES_USER:-temporal}\n      - POSTGRES_PWD=${TEMPORAL_POSTGRES_PASSWORD:-change-me-in-production}\n      - POSTGRES_SEEDS=postgres-temporal\n      - POSTGRES_DATABASE=${TEMPORAL_POSTGRES_DB:-temporal}\n      - NAMESPACES=default,${TEMPORAL_NAMESPACE:-production}\n      - HISTORY_NAMESPACE=${TEMPORAL_NAMESPACE:-production}\n      - FRONTEND_ADDRESS=${TEMPORAL_ADDRESS:-0.0.0.0:7233}\n      - MEMBERSHIP_PORT=6933\n      - RPC_QUEUE_TYPE=normal\n      - PUBLIC_IP=${TEMPORAL_PUBLIC_IP:-localhost}\n      - SERVE_FRONTEND=true\n      - SERVE_HISTORY=false\n      - SERVE_MATCHER=false\n      - SERVE_WORKER=false\n      - CLUSTER_NAME=${TEMPORAL_CLUSTER_NAME:-legal-ai-temporal}\n      - TLS_ENABLED=${TEMPORAL_TLS_ENABLED:-false}\n      - TLS_SERVER_NAME=${TEMPORAL_TLS_SERVER_NAME:-}\n      - TLS_CERT_FILE=${TEMPORAL_TLS_CERT_FILE:-}\n      - TLS_KEY_FILE=${TEMPORAL_TLS_KEY_FILE:-}\n      - TLS_CLIENT_CA_FILE=${TEMPORAL_TLS_CLIENT_CA_FILE:-}\n      - TLS_INT_SERVER_NAME=${TEMPORAL_TLS_INT_SERVER_NAME:-}\n      - TLS_INT_CERT_FILE=${TEMPORAL_TLS_INT_CERT_FILE:-}\n      - TLS_INT_KEY_FILE=${TEMPORAL_TLS_INT_KEY_FILE:-}\n      - TLS_INT_CLIENT_CA_FILE=${TEMPORAL_TLS_INT_CLIENT_CA_FILE:-}\n      - PROMETHEUS_ENDPOINT=/metrics\n      - PROMETHEUS_LISTEN_ADDRESS=:9090\n      # Elasticsearch for visibility\n      - VISIBILITY_ENABLE=true\n      - ES_SEEDS=elasticsearch\n      - ES_PORT=9200\n      - ES_VERSION=v7\n      - ES_VIS_INDEX=visibility-dev\n      # Archival configuration\n      - HISTORY_ENABLE_EAGER_READ=false\n    depends_on:\n      postgres-temporal:\n        condition: service_healthy\n      elasticsearch:\n        condition: service_healthy\n    command: ['temporal-server', 'start']\n    restart: always\n    healthcheck:\n      test: ['CMD', 'tctl', '--address', 'temporal-frontend:7233', 'cluster', 'health']\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 30s\n    networks:\n      - temporal-network\n\n  # --------------------------------------------------------------------------\n  # Temporal History Service (Replicated)\n  # --------------------------------------------------------------------------\n  temporal-history:\n    image: temporalio/server:latest\n    container_name: temporal-history\n    environment:\n      - SERVICES=history\n      - DB=postgresql\n      - DB_PORT=5432\n      - POSTGRES_USER=${TEMPORAL_POSTGRES_USER:-temporal}\n      - POSTGRES_PWD=${TEMPORAL_POSTGRES_PASSWORD:-change-me-in-production}\n      - POSTGRES_SEEDS=postgres-temporal\n      - POSTGRES_DATABASE=${TEMPORAL_POSTGRES_DB:-temporal}\n      - NAMESPACES=default,${TEMPORAL_NAMESPACE:-production}\n      - HISTORY_NAMESPACE=${TEMPORAL_NAMESPACE:-production}\n      - FRONTEND_ADDRESS=temporal-frontend:7233\n      - MASTER_ADDRESS=temporal-frontend:7233\n      - HISTORY_ADDRESS=0.0.0.0:7234\n      - MEMBERSHIP_PORT=6934\n      - RPC_QUEUE_TYPE=normal\n      - NUM_HISTORY_SHARDS=${TEMPORAL_HISTORY_SHARDS:-512}\n      - HISTORY_ENABLE_EAGER_READ=false\n      - SERVE_FRONTEND=false\n      - SERVE_HISTORY=true\n      - SERVE_MATCHER=false\n      - SERVE_WORKER=false\n      - CLUSTER_NAME=${TEMPORAL_CLUSTER_NAME:-legal-ai-temporal}\n      - TLS_ENABLED=${TEMPORAL_TLS_ENABLED:-false}\n      - TLS_INT_SERVER_NAME=${TEMPORAL_TLS_INT_SERVER_NAME:-}\n      - TLS_INT_CERT_FILE=${TEMPORAL_TLS_INT_CERT_FILE:-}\n      - TLS_INT_KEY_FILE=${TEMPORAL_TLS_INT_KEY_FILE:-}\n      - TLS_INT_CLIENT_CA_FILE=${TEMPORAL_TLS_INT_CLIENT_CA_FILE:-}\n      - PROMETHEUS_ENDPOINT=/metrics\n      - PROMETHEUS_LISTEN_ADDRESS=:9091\n    depends_on:\n      postgres-temporal:\n        condition: service_healthy\n    command: ['temporal-server', 'start']\n    restart: always\n    healthcheck:\n      test: ['CMD-SHELL', 'curl -f http://localhost:9091/metrics || exit 1']\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 30s\n    networks:\n      - temporal-network\n\n  # --------------------------------------------------------------------------\n  # Temporal Matching Service\n  # --------------------------------------------------------------------------\n  temporal-matching:\n    image: temporalio/server:latest\n    container_name: temporal-matching\n    environment:\n      - SERVICES=matching\n      - DB=postgresql\n      - DB_PORT=5432\n      - POSTGRES_USER=${TEMPORAL_POSTGRES_USER:-temporal}\n      - POSTGRES_PWD=${TEMPORAL_POSTGRES_PASSWORD:-change-me-in-production}\n      - POSTGRES_SEEDS=postgres-temporal\n      - POSTGRES_DATABASE=${TEMPORAL_POSTGRES_DB:-temporal}\n      - NAMESPACES=default,${TEMPORAL_NAMESPACE:-production}\n      - MATCHING_NAMESPACE=${TEMPORAL_NAMESPACE:-production}\n      - FRONTEND_ADDRESS=temporal-frontend:7233\n      - MASTER_ADDRESS=temporal-frontend:7233\n      - MATCHING_ADDRESS=0.0.0.0:7235\n      - MEMBERSHIP_PORT=6935\n      - RPC_QUEUE_TYPE=normal\n      - SERVE_FRONTEND=false\n      - SERVE_HISTORY=false\n      - SERVE_MATCHER=true\n      - SERVE_WORKER=false\n      - CLUSTER_NAME=${TEMPORAL_CLUSTER_NAME:-legal-ai-temporal}\n      - TLS_ENABLED=${TEMPORAL_TLS_ENABLED:-false}\n      - TLS_INT_SERVER_NAME=${TEMPORAL_TLS_INT_SERVER_NAME:-}\n      - TLS_INT_CERT_FILE=${TEMPORAL_TLS_INT_CERT_FILE:-}\n      - TLS_INT_KEY_FILE=${TEMPORAL_TLS_INT_KEY_FILE:-}\n      - TLS_INT_CLIENT_CA_FILE=${TEMPORAL_TLS_INT_CLIENT_CA_FILE:-}\n      - PROMETHEUS_ENDPOINT=/metrics\n      - PROMETHEUS_LISTEN_ADDRESS=:9092\n    depends_on:\n      postgres-temporal:\n        condition: service_healthy\n    command: ['temporal-server', 'start']\n    restart: always\n    healthcheck:\n      test: ['CMD-SHELL', 'curl -f http://localhost:9092/metrics || exit 1']\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 30s\n    networks:\n      - temporal-network\n\n  # --------------------------------------------------------------------------\n  # Temporal Worker Service\n  # --------------------------------------------------------------------------\n  temporal-worker:\n    image: temporalio/server:latest\n    container_name: temporal-worker\n    environment:\n      - SERVICES=worker\n      - DB=postgresql\n      - DB_PORT=5432\n      - POSTGRES_USER=${TEMPORAL_POSTGRES_USER:-temporal}\n      - POSTGRES_PWD=${TEMPORAL_POSTGRES_PASSWORD:-change-me-in-production}\n      - POSTGRES_SEEDS=postgres-temporal\n      - POSTGRES_DATABASE=${TEMPORAL_POSTGRES_DB:-temporal}\n      - NAMESPACES=default,${TEMPORAL_NAMESPACE:-production}\n      - FRONTEND_ADDRESS=temporal-frontend:7233\n      - MASTER_ADDRESS=temporal-frontend:7233\n      - WORKER_ADDRESS=0.0.0.0:7236\n      - MEMBERSHIP_PORT=6936\n      - RPC_QUEUE_TYPE=normal\n      - SERVE_FRONTEND=false\n      - SERVE_HISTORY=false\n      - SERVE_MATCHER=false\n      - SERVE_WORKER=true\n      - CLUSTER_NAME=${TEMPORAL_CLUSTER_NAME:-legal-ai-temporal}\n      - TLS_ENABLED=${TEMPORAL_TLS_ENABLED:-false}\n      - TLS_INT_SERVER_NAME=${TEMPORAL_TLS_INT_SERVER_NAME:-}\n      - TLS_INT_CERT_FILE=${TEMPORAL_TLS_INT_CERT_FILE:-}\n      - TLS_INT_KEY_FILE=${TEMPORAL_TLS_INT_KEY_FILE:-}\n      - TLS_INT_CLIENT_CA_FILE=${TEMPORAL_TLS_INT_CLIENT_CA_FILE:-}\n      - PROMETHEUS_ENDPOINT=/metrics\n      - PROMETHEUS_LISTEN_ADDRESS=:9093\n    depends_on:\n      postgres-temporal:\n        condition: service_healthy\n    command: ['temporal-server', 'start']\n    restart: always\n    healthcheck:\n      test: ['CMD-SHELL', 'curl -f http://localhost:9093/metrics || exit 1']\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 30s\n    networks:\n      - temporal-network\n\n  # --------------------------------------------------------------------------\n  # Temporal Web UI with Authentication\n  # --------------------------------------------------------------------------\n  temporal-ui:\n    image: temporalio/ui:latest\n    container_name: temporal-ui\n    ports:\n      - '${TEMPORAL_UI_PORT:-8088}:8088'\n    environment:\n      - TEMPORAL_ADDRESS=temporal-frontend:7233\n      - TEMPORAL_CORS_ORIGINS=${TEMPORAL_CORS_ORIGINS:-http://localhost:3000,http://localhost:3001}\n      - TEMPORAL_UI_ENABLED=${TEMPORAL_UI_ENABLED:-true}\n      # Authentication (using OAuth2 proxy)\n      - OAUTH2_PROXY_ENABLED=${TEMPORAL_UI_OAUTH_ENABLED:-false}\n      - OAUTH2_PROXY_CLIENT_ID=${TEMPORAL_UI_OAUTH_CLIENT_ID:-}\n      - OAUTH2_PROXY_CLIENT_SECRET=${TEMPORAL_UI_OAUTH_CLIENT_SECRET:-}\n      - OAUTH2_PROXY_COOKIE_SECRET=${TEMPORAL_UI_OAUTH_COOKIE_SECRET:-}\n      - OAUTH2_PROXY_EMAIL_DOMAIN=${TEMPORAL_UI_OAUTH_EMAIL_DOMAIN:-*}\n    depends_on:\n      temporal-frontend:\n        condition: service_healthy\n    restart: always\n    healthcheck:\n      test: ['CMD-SHELL', 'curl -f http://localhost:8088/health || exit 1']\n      interval: 10s\n      timeout: 5s\n      retries: 3\n    networks:\n      - temporal-network\n\n  # --------------------------------------------------------------------------\n  # Prometheus - Metrics Collection\n  # --------------------------------------------------------------------------\n  prometheus:\n    image: prom/prometheus:latest\n    container_name: temporal-prometheus\n    ports:\n      - '9090:9090'\n    volumes:\n      - ./apps/backend/src/modules/temporal/config/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n      - prometheus_data:/prometheus\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--web.console.libraries=/usr/share/prometheus/console_libraries'\n      - '--web.console.templates=/usr/share/prometheus/consoles'\n    restart: always\n    healthcheck:\n      test: ['CMD-SHELL', 'wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1']\n      interval: 10s\n      timeout: 5s\n      retries: 3\n    networks:\n      - temporal-network\n\n  # --------------------------------------------------------------------------\n  # Grafana - Metrics Visualization\n  # --------------------------------------------------------------------------\n  grafana:\n    image: grafana/grafana:latest\n    container_name: temporal-grafana\n    ports:\n      - '3001:3000'\n    environment:\n      - GF_SECURITY_ADMIN_USER=${TEMPORAL_GRAFANA_USER:-admin}\n      - GF_SECURITY_ADMIN_PASSWORD=${TEMPORAL_GRAFANA_PASSWORD:-change-me-in-production}\n      - GF_INSTALL_PLUGINS=\n      - GF_SERVER_ROOT_URL=http://localhost:3001\n      - GF_ANALYTICS_REPORTING_ENABLED=false\n      - GF_ANALYTICS_CHECK_FOR_UPDATES=false\n    volumes:\n      - grafana_data:/var/lib/grafana\n      - ./apps/backend/src/modules/temporal/config/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro\n      - ./apps/backend/src/modules/temporal/config/grafana-dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml:ro\n      - ./apps/backend/src/modules/temporal/config/grafana-dashboards:/var/lib/grafana/dashboards:ro\n    restart: always\n    healthcheck:\n      test: ['CMD-SHELL', 'curl -f http://localhost:3000/api/health || exit 1']\n      interval: 10s\n      timeout: 5s\n      retries: 3\n    networks:\n      - temporal-network\n\n  # --------------------------------------------------------------------------\n  # AlertManager - Alerting for Prometheus\n  # --------------------------------------------------------------------------\n  alertmanager:\n    image: prom/alertmanager:latest\n    container_name: temporal-alertmanager\n    ports:\n      - '9093:9093'\n    volumes:\n      - ./apps/backend/src/modules/temporal/config/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro\n      - alertmanager_data:/alertmanager\n    command:\n      - '--config.file=/etc/alertmanager/alertmanager.yml'\n      - '--storage.path=/alertmanager'\n    restart: always\n    networks:\n      - temporal-network\n\nvolumes:\n  temporal_postgres_data:\n    driver: local\n  temporal_elasticsearch_data:\n    driver: local\n  prometheus_data:\n    driver: local\n  grafana_data:\n    driver: local\n  alertmanager_data:\n    driver: local\n\nnetworks:\n  temporal-network:\n    driver: bridge\n"
}
Now let me create the Prometheus configuration and other supporting configs:

ðŸ”§ Tool: Bash
Input: {
  "command": "mkdir -p /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/config",
  "description": "Create temporal config directory"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/config/prometheus.yml",
  "content": "# ============================================================================\n# Prometheus Configuration for Temporal Cluster\n# ============================================================================\n\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    cluster: 'legal-ai-temporal'\n    environment: 'production'\n\n# Alerting configuration\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n            - alertmanager:9093\n\n# Rule files\nrule_files:\n  - '/etc/prometheus/temporal-alerts.yml'\n\n# Scrape configurations\nscrape_configs:\n  # Temporal Frontend Service Metrics\n  - job_name: 'temporal-frontend'\n    static_configs:\n      - targets:\n          - temporal-frontend:9090\n        labels:\n          service: 'temporal-frontend'\n\n  # Temporal History Service Metrics\n  - job_name: 'temporal-history'\n    static_configs:\n      - targets:\n          - temporal-history:9091\n        labels:\n          service: 'temporal-history'\n\n  # Temporal Matching Service Metrics\n  - job_name: 'temporal-matching'\n    static_configs:\n      - targets:\n          - temporal-matching:9092\n        labels:\n          service: 'temporal-matching'\n\n  # Temporal Worker Service Metrics\n  - job_name: 'temporal-worker'\n    static_configs:\n      - targets:\n          - temporal-worker:9093\n        labels:\n          service: 'temporal-worker'\n\n  # Prometheus self-monitoring\n  - job_name: 'prometheus'\n    static_configs:\n      - targets:\n          - localhost:9090\n\n  # PostgreSQL Exporter (optional - add postgres-exporter container if needed)\n  - job_name: 'postgres-temporal'\n    static_configs:\n      - targets:\n          - postgres-exporter:9187\n        labels:\n          service: 'postgres-temporal'\n\n  # Elasticsearch Exporter (optional)\n  - job_name: 'elasticsearch'\n    static_configs:\n      - targets:\n          - elasticsearch-exporter:9114\n        labels:\n          service: 'elasticsearch'\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/config/alertmanager.yml",
  "content": "# ============================================================================\n# AlertManager Configuration for Temporal Cluster\n# ============================================================================\n\nglobal:\n  resolve_timeout: 5m\n  slack_api_url: '${ALERTMANAGER_SLACK_API_URL:-}'\n  smtp_smarthost: '${ALERTMANAGER_SMARTHOST:-localhost:587}'\n  smtp_from: '${ALERTMANAGER_SMTP_FROM:-alertmanager@example.com}'\n  smtp_auth_username: '${ALERTMANAGER_SMTP_USERNAME:-}'\n  smtp_auth_password: '${ALERTMANAGER_SMTP_PASSWORD:-}'\n\n# Route configuration\nroute:\n  group_by: ['alertname', 'cluster', 'service']\n  group_wait: 10s\n  group_interval: 10s\n  repeat_interval: 12h\n  receiver: 'default'\n  routes:\n    - match:\n        severity: critical\n      receiver: 'critical-alerts'\n    - match:\n        severity: warning\n      receiver: 'warning-alerts'\n\n# Receivers configuration\nreceivers:\n  - name: 'default'\n    slack_configs:\n      - channel: '#temporal-alerts'\n        title: 'Temporal Alert: {{ .GroupLabels.alertname }}'\n        text: >-\n          {{ range .Alerts }}\n          *Alert:* {{ .Labels.alertname }}\n          *Severity:* {{ .Labels.severity }}\n          *Service:* {{ .Labels.service }}\n          *Summary:* {{ .Annotations.summary }}\n          *Description:* {{ .Annotations.description }}\n          {{ end }}\n\n  - name: 'critical-alerts'\n    slack_configs:\n      - channel: '#temporal-critical'\n        send_resolved: true\n        title: 'ðŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'\n        color: '{{ if eq .Status \"firing\" }}danger{{ else }}good{{ end }}'\n        text: >-\n          {{ range .Alerts }}\n          *Alert:* {{ .Labels.alertname }}\n          *Severity:* {{ .Labels.severity }}\n          *Service:* {{ .Labels.service }}\n          *Summary:* {{ .Annotations.summary }}\n          *Description:* {{ .Annotations.description }}\n          {{ end }}\n    email_configs:\n      - to: '${ALERTMANAGER_CRITICAL_EMAIL:-ops@example.com}'\n        send_resolved: true\n\n  - name: 'warning-alerts'\n    slack_configs:\n      - channel: '#temporal-warnings'\n        send_resolved: true\n        title: 'âš ï¸ WARNING: {{ .GroupLabels.alertname }}'\n        color: 'warning'\n        text: >-\n          {{ range .Alerts }}\n          *Alert:* {{ .Labels.alertname }}\n          *Severity:* {{ .Labels.severity }}\n          *Service:* {{ .Labels.service }}\n          *Summary:* {{ .Annotations.summary }}\n          {{ end }}\n\n# Inhibition rules\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'cluster']\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/config/prometheus-alerts.yml",
  "content": "# ============================================================================\n# Prometheus Alert Rules for Temporal Cluster\n# ============================================================================\n\ngroups:\n  - name: temporal_alerts\n    interval: 30s\n    rules:\n      # Frontend service down\n      - alert: TemporalFrontendDown\n        expr: up{job=\"temporal-frontend\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: temporal-frontend\n        annotations:\n          summary: 'Temporal Frontend service is down'\n          description: 'Temporal Frontend service has been down for more than 1 minute.'\n\n      # History service down\n      - alert: TemporalHistoryDown\n        expr: up{job=\"temporal-history\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: temporal-history\n        annotations:\n          summary: 'Temporal History service is down'\n          description: 'Temporal History service has been down for more than 1 minute.'\n\n      # Matching service down\n      - alert: TemporalMatchingDown\n        expr: up{job=\"temporal-matching\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          service: temporal-matching\n        annotations:\n          summary: 'Temporal Matching service is down'\n          description: 'Temporal Matching service has been down for more than 1 minute.'\n\n      # Worker service down\n      - alert: TemporalWorkerDown\n        expr: up{job=\"temporal-worker\"} == 0\n        for: 2m\n        labels:\n          severity: warning\n          service: temporal-worker\n        annotations:\n          summary: 'Temporal Worker service is down'\n          description: 'Temporal Worker service has been down for more than 2 minutes.'\n\n      # High workflow failure rate\n      - alert: HighWorkflowFailureRate\n        expr: |\n          rate(temporal_workflow_failed[5m]) / rate(temporal_workflow_completed[5m]) > 0.1\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'High workflow failure rate detected'\n          description: 'Workflow failure rate is above 10% for the last 5 minutes.'\n\n      # High activity failure rate\n      - alert: HighActivityFailureRate\n        expr: |\n          rate(temporal_activity_failed[5m]) / rate(temporal_activity_completed[5m]) > 0.15\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'High activity failure rate detected'\n          description: 'Activity failure rate is above 15% for the last 5 minutes.'\n\n      # Workflow execution timeout\n      - alert: WorkflowExecutionTimeout\n        expr: |\n          rate(temporal_workflow_timed_out[5m]) > 0.05\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'High workflow timeout rate'\n          description: 'More than 5% of workflows are timing out.'\n\n      # Service latency high\n      - alert: TemporalServiceLatencyHigh\n        expr: |\n          histogram_quantile(0.99, rate(temporal_request_latency_bucket[5m])) > 1\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'Temporal service latency is high'\n          description: 'P99 latency is above 1 second for the last 5 minutes.'\n\n      # Database connection pool exhausted\n      - alert: DatabaseConnectionPoolExhausted\n        expr: |\n          temporal_sql_connections{state=\"idle\"} < 5\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: 'Database connection pool nearly exhausted'\n          description: 'Less than 5 idle database connections available.'\n\n      # Elasticsearch down\n      - alert: ElasticsearchDown\n        expr: up{job=\"elasticsearch\"} == 0\n        for: 2m\n        labels:\n          severity: critical\n          service: elasticsearch\n        annotations:\n          summary: 'Elasticsearch is down'\n          description: 'Elasticsearch has been down for more than 2 minutes. Visibility will be affected.'\n\n      # Disk space low\n      - alert: DiskSpaceLow\n        expr: |\n          (node_filesystem_avail_bytes{mountpoint=\"/\"} / node_filesystem_size_bytes{mountpoint=\"/\"}) < 0.1\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'Disk space is low'\n          description: 'Less than 10% disk space available on temporal server.'\n\n  - name: temporal_resource_alerts\n    interval: 1m\n    rules:\n      # High memory usage\n      - alert: TemporalHighMemoryUsage\n        expr: |\n          (container_memory_usage_bytes{name=~\"temporal-.*\"} / container_spec_memory_limit_bytes{name=~\"temporal-.*\"}) > 0.9\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'Temporal service high memory usage'\n          description: 'Temporal service {{ $labels.name }} is using more than 90% of its memory limit.'\n\n      # High CPU usage\n      - alert: TemporalHighCPUUsage\n        expr: |\n          rate(container_cpu_usage_seconds_total{name=~\"temporal-.*\"}[5m]) > 0.8\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'Temporal service high CPU usage'\n          description: 'Temporal service {{ $labels.name }} is using more than 80% CPU for 10 minutes.'\n"
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Exploring existing Temporal setup",
      "content": "Explore and understand existing Temporal setup",
      "status": "completed"
    },
    {
      "activeForm": "Creating production Docker Compose configuration",
      "content": "Create production Docker Compose configuration for Temporal cluster",
      "status": "completed"
    },
    {
      "activeForm": "Creating Kubernetes manifests",
      "content": "Create Kubernetes manifests for Temporal deployment",
      "status": "in_progress"
    },
    {
      "activeForm": "Setting up TLS/SSL configuration",
      "content": "Set up TLS/SSL configuration for Temporal services",
      "status": "pending"
    },
    {
      "activeForm": "Configuring Prometheus metrics",
      "content": "Configure Prometheus metrics for Temporal",
      "status": "completed"
    },
    {
      "activeForm": "Setting up Temporal UI with authentication",
      "content": "Set up Temporal UI with authentication",
      "status": "pending"
    },
    {
      "activeForm": "Implementing health checks and monitoring",
      "content": "Implement health checks and monitoring",
      "status": "pending"
    },
    {
      "activeForm": "Configuring archival for workflow history",
      "content": "Configure archival for workflow history",
      "status": "pending"
    },
    {
      "activeForm": "Creating deployment documentation",
      "content": "Create deployment documentation",
      "status": "pending"
    }
  ]
}
Now let me create Grafana configurations and then move on to Kubernetes manifests:

ðŸ”§ Tool: Bash
Input: {
  "command": "mkdir -p /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/config/grafana-dashboards",
  "description": "Create Grafana dashboards directory"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/config/grafana-datasources.yml",
  "content": "# ============================================================================\n# Grafana Datasources Provisioning Configuration\n# ============================================================================\n\napiVersion: 1\n\ndatasources:\n  - name: Prometheus\n    type: prometheus\n    access: proxy\n    url: http://prometheus:9090\n    isDefault: true\n    editable: true\n    jsonData:\n      timeInterval: 15s\n      queryTimeout: 60s\n      httpMethod: POST\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/config/grafana-dashboards.yml",
  "content": "# ============================================================================\n# Grafana Dashboards Provisioning Configuration\n# ============================================================================\n\napiVersion: 1\n\nproviders:\n  - name: 'Temporal'\n    orgId: 1\n    folder: 'Temporal'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 10\n    allowUiUpdates: true\n    options:\n      path: /var/lib/grafana/dashboards\n      foldersFromFilesStructure: true\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/config/grafana-dashboards/temporal-overview.json",
  "content": "{\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": \"-- Grafana --\",\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n  \"editable\": true,\n  \"gnetId\": null,\n  \"graphTooltip\": 0,\n  \"id\": null,\n  \"links\": [],\n  \"panels\": [\n    {\n      \"datasource\": \"Prometheus\",\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"thresholds\"\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"red\",\n                \"value\": null\n              },\n              {\n                \"color\": \"green\",\n                \"value\": 1\n              }\n            ]\n          },\n          \"unit\": \"none\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 4,\n        \"w\": 4,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"id\": 1,\n      \"options\": {\n        \"colorMode\": \"background\",\n        \"graphMode\": \"none\",\n        \"justifyMode\": \"auto\",\n        \"orientation\": \"auto\",\n        \"reduceOptions\": {\n          \"calcs\": [\"lastNotNull\"],\n          \"fields\": \"\",\n          \"values\": false\n        },\n        \"textMode\": \"value\"\n      },\n      \"pluginVersion\": \"9.0.0\",\n      \"targets\": [\n        {\n          \"expr\": \"up{job=~\\\"temporal-.*\\\"}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Services Up\",\n      \"type\": \"stat\"\n    },\n    {\n      \"datasource\": \"Prometheus\",\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"drawStyle\": \"line\",\n            \"fillOpacity\": 10,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"tooltip\": false,\n              \"viz\": false,\n              \"legend\": false\n            },\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"never\",\n            \"spanNulls\": true\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": null\n              }\n            ]\n          },\n          \"unit\": \"ops\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 4\n      },\n      \"id\": 2,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [],\n          \"displayMode\": \"list\",\n          \"placement\": \"bottom\"\n        },\n        \"tooltip\": {\n          \"mode\": \"single\"\n        }\n      },\n      \"pluginVersion\": \"9.0.0\",\n      \"targets\": [\n        {\n          \"expr\": \"rate(temporal_workflow_completed[5m])\",\n          \"legendFormat\": \"Completed - {{job}}\",\n          \"refId\": \"A\"\n        },\n        {\n          \"expr\": \"rate(temporal_workflow_failed[5m])\",\n          \"legendFormat\": \"Failed - {{job}}\",\n          \"refId\": \"B\"\n        }\n      ],\n      \"title\": \"Workflow Throughput\",\n      \"type\": \"timeseries\"\n    },\n    {\n      \"datasource\": \"Prometheus\",\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"color\": {\n            \"mode\": \"palette-classic\"\n          },\n          \"custom\": {\n            \"axisLabel\": \"\",\n            \"axisPlacement\": \"auto\",\n            \"barAlignment\": 0,\n            \"drawStyle\": \"line\",\n            \"fillOpacity\": 10,\n            \"gradientMode\": \"none\",\n            \"hideFrom\": {\n              \"tooltip\": false,\n              \"viz\": false,\n              \"legend\": false\n            },\n            \"lineInterpolation\": \"linear\",\n            \"lineWidth\": 1,\n            \"pointSize\": 5,\n            \"scaleDistribution\": {\n              \"type\": \"linear\"\n            },\n            \"showPoints\": \"never\",\n            \"spanNulls\": true\n          },\n          \"mappings\": [],\n          \"thresholds\": {\n            \"mode\": \"absolute\",\n            \"steps\": [\n              {\n                \"color\": \"green\",\n                \"value\": null\n              }\n            ]\n          },\n          \"unit\": \"ops\"\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 4\n      },\n      \"id\": 3,\n      \"options\": {\n        \"legend\": {\n          \"calcs\": [],\n          \"displayMode\": \"list\",\n          \"placement\": \"bottom\"\n        },\n        \"tooltip\": {\n          \"mode\": \"single\"\n        }\n      },\n      \"pluginVersion\": \"9.0.0\",\n      \"targets\": [\n        {\n          \"expr\": \"rate(temporal_activity_completed[5m])\",\n          \"legendFormat\": \"Activities - {{job}}\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"title\": \"Activity Throughput\",\n      \"type\": \"timeseries\"\n    }\n  ],\n  \"schemaVersion\": 36,\n  \"style\": \"dark\",\n  \"tags\": [\"temporal\"],\n  \"templating\": {\n    \"list\": []\n  },\n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n  \"timepicker\": {},\n  \"timezone\": \"\",\n  \"title\": \"Temporal Cluster Overview\",\n  \"uid\": \"temporal-overview\",\n  \"version\": 1\n}\n"
}
Now let me create the Kubernetes manifests directory and the manifests for Temporal deployment:

ðŸ”§ Tool: Bash
Input: {
  "command": "mkdir -p /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/{base,overlays/production,overlays/staging}",
  "description": "Create k8s directories"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/kustomization.yml",
  "content": "# ============================================================================\n# Kustomization Base Configuration for Temporal Cluster\n# ============================================================================\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: temporal\n\n# Resources\nresources:\n  - namespace.yaml\n  - configmap.yaml\n  - secrets.yaml\n  - postgres.yaml\n  - elasticsearch.yaml\n  - frontend.yaml\n  - history.yaml\n  - matching.yaml\n  - worker.yaml\n  - ui.yaml\n  - prometheus.yaml\n  - grafana.yaml\n  - alertmanager.yaml\n\n# Common labels applied to all resources\ncommonLabels:\n  app.kubernetes.io/name: temporal\n  app.kubernetes.io/part-of: temporal-cluster\n  app.kubernetes.io/managed-by: kustomize\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/namespace.yaml",
  "content": "# ============================================================================\n# Temporal Namespace\n# ============================================================================\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: temporal\n  labels:\n    name: temporal\n    app.kubernetes.io/name: temporal\n    app.kubernetes.io/part-of: temporal-cluster\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/configmap.yaml",
  "content": "# ============================================================================\n# Temporal ConfigMap\n# ============================================================================\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: temporal-config\n  namespace: temporal\ndata:\n  # Temporal server configuration\n  temporal.yaml: |\n    cluster:\n      name: legal-ai-temporal\n      initialize: true\n\n    frontend:\n      address: \"0.0.0.0:7233\"\n      membership:\n        address: \"0.0.0.0:6933\"\n      rps:\n        # Requests per second limits\n        global:\n          frontend: 2000\n          history: 2000\n          matching: 2000\n          worker: 2000\n\n    history:\n      address: \"0.0.0.0:7234\"\n      membership:\n        address: \"0.0.0.0:6934\"\n      numHistoryShards: 512\n      historyCacheInitialSize: 1000\n      historyCacheMaxSize: 10000\n      historyCachePersistTransactionKey: false\n\n    matching:\n      address: \"0.0.0.0:7235\"\n      membership:\n        address: \"0.0.0.0:6935\"\n\n    worker:\n      address: \"0.0.0.0:7236\"\n      membership:\n        address: \"0.0.0.0:6936\"\n\n    publicClient:\n      hostPort: \"0.0.0.0:7233\"\n\n    # Metrics configuration\n    metrics:\n      tags:\n        environment: production\n        cluster: legal-ai-temporal\n      prometheus:\n        listenAddress: \"0.0.0.0:9090\"\n      perEndpointHistogram:\n        enabled: true\n\n    # Namespace configuration\n    namespaces:\n      - name: default\n        clusters:\n          - legal-ai-temporal\n        isGlobalNamespace: true\n\n    # Replication configuration for HA\n    replication:\n      enable: true\n      initialFailoverVersion: 0\n      maxWorkflowTimeoutSeconds: 31536000  # 1 year\n\n    # Elasticsearch for visibility\n    elasticsearch:\n      visibility:\n        index: temporal-visibility-dev\n        visibilityIndex: temporal-visibility-dev\n        url:\n          - elasticsearch:9200\n        indices:\n          visibility: visibility\n        enableElasticsearchVisibility: true\n        esType: \"\"\n        username: \"\"\n        password: \"\"\n        version: v7\n\n  # Prometheus configuration\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n      evaluation_interval: 15s\n\n    scrape_configs:\n      - job_name: 'temporal-frontend'\n        kubernetes_sd_configs:\n          - role: pod\n            namespaces:\n              names:\n                - temporal\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]\n            regex: frontend\n            action: keep\n          - source_labels: [__meta_kubernetes_pod_ip]\n            target_label: __address__\n            replacement: $1:9090\n\n      - job_name: 'temporal-history'\n        kubernetes_sd_configs:\n          - role: pod\n            namespaces:\n              names:\n                - temporal\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]\n            regex: history\n            action: keep\n          - source_labels: [__meta_kubernetes_pod_ip]\n            target_label: __address__\n            replacement: $1:9091\n\n      - job_name: 'temporal-matching'\n        kubernetes_sd_configs:\n          - role: pod\n            namespaces:\n              names:\n                - temporal\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]\n            regex: matching\n            action: keep\n          - source_labels: [__meta_kubernetes_pod_ip]\n            target_label: __address__\n            replacement: $1:9092\n\n      - job_name: 'temporal-worker'\n        kubernetes_sd_configs:\n          - role: pod\n            namespaces:\n              names:\n                - temporal\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]\n            regex: worker\n            action: keep\n          - source_labels: [__meta_kubernetes_pod_ip]\n            target_label: __address__\n            replacement: $1:9093\n\n  # Alert configuration\n  temporal-alerts.yml: |\n    groups:\n      - name: temporal_alerts\n        interval: 30s\n        rules:\n          - alert: TemporalFrontendDown\n            expr: up{job=\"temporal-frontend\"} == 0\n            for: 1m\n            labels:\n              severity: critical\n            annotations:\n              summary: \"Temporal Frontend service is down\"\n\n          - alert: TemporalHistoryDown\n            expr: up{job=\"temporal-history\"} == 0\n            for: 1m\n            labels:\n              severity: critical\n            annotations:\n              summary: \"Temporal History service is down\"\n\n          - alert: HighWorkflowFailureRate\n            expr: |\n              rate(temporal_workflow_failed[5m]) / rate(temporal_workflow_completed[5m]) > 0.1\n            for: 5m\n            labels:\n              severity: warning\n            annotations:\n              summary: \"High workflow failure rate detected\"\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/secrets.yaml",
  "content": "# ============================================================================\n# Temporal Secrets\n# ============================================================================\napiVersion: v1\nkind: Secret\nmetadata:\n  name: temporal-secrets\n  namespace: temporal\ntype: Opaque\nstringData:\n  # PostgreSQL credentials\n  POSTGRES_USER: temporal\n  POSTGRES_PASSWORD: change-me-in-production\n  POSTGRES_DATABASE: temporal\n\n  # TLS certificates (empty for now, populate in production overlay)\n  TLS_SERVER_NAME: \"\"\n  TLS_CERT: \"\"\n  TLS_KEY: \"\"\n  TLS_CLIENT_CA: \"\"\n\n  # Grafana credentials\n  GRAFANA_ADMIN_USER: admin\n  GRAFANA_ADMIN_PASSWORD: change-me-in-production\n\n  # AlertManager configuration\n  ALERTMANAGER_SLACK_API_URL: \"\"\n\n---\n# Secret for TLS certificates (optional, for production)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: temporal-tls\n  namespace: temporal\ntype: kubernetes.io/tls\ndata:\n  # Base64 encoded certificates\n  # Populate these with real certificates in production\n  tls.crt: LS0tLS... # Base64 encoded certificate\n  tls.key: LS0tLS... # Base64 encoded private key\n  ca.crt: LS0tLS... # Base64 encoded CA certificate\n\n---\n# Secret for internal TLS (optional)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: temporal-tls-internal\n  namespace: temporal\ntype: kubernetes.io/tls\ndata:\n  # Base64 encoded certificates for internal communication\n  tls.crt: LS0tLS...\n  tls.key: LS0tLS...\n  ca.crt: LS0tLS...\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/postgres.yaml",
  "content": "# ============================================================================\n# PostgreSQL StatefulSet for Temporal\n# ============================================================================\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: temporal-postgres-pvc\n  namespace: temporal\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: fast-ssd\n  resources:\n    requests:\n      storage: 50Gi\n\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: temporal-postgres\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: postgres\n    app.kubernetes.io/component: database\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  serviceName: temporal-postgres\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: postgres\n      app.kubernetes.io/component: database\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: postgres\n        app.kubernetes.io/component: database\n        app.kubernetes.io/part-of: temporal-cluster\n    spec:\n      containers:\n        - name: postgres\n          image: postgres:15-alpine\n          ports:\n            - containerPort: 5432\n              name: postgres\n          env:\n            - name: POSTGRES_USER\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_USER\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_PASSWORD\n            - name: POSTGRES_DB\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_DATABASE\n            - name: PGDATA\n              value: /var/lib/postgresql/data/pgdata\n          volumeMounts:\n            - name: postgres-data\n              mountPath: /var/lib/postgresql/data\n          resources:\n            requests:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n            limits:\n              memory: \"4Gi\"\n              cpu: \"2000m\"\n          livenessProbe:\n            exec:\n              command:\n                - pg_isready\n                - -U\n                - $(POSTGRES_USER)\n                - -d\n                - $(POSTGRES_DB)\n            initialDelaySeconds: 30\n            periodSeconds: 10\n          readinessProbe:\n            exec:\n              command:\n                - pg_isready\n                - -U\n                - $(POSTGRES_USER)\n                - -d\n                - $(POSTGRES_DB)\n            initialDelaySeconds: 5\n            periodSeconds: 5\n      volumes:\n        - name: postgres-data\n          persistentVolumeClaim:\n            claimName: temporal-postgres-pvc\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: temporal-postgres\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: postgres\n    app.kubernetes.io/component: database\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  type: ClusterIP\n  ports:\n    - port: 5432\n      targetPort: 5432\n      name: postgres\n  selector:\n    app.kubernetes.io/name: postgres\n    app.kubernetes.io/component: database\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/elasticsearch.yaml",
  "content": "# ============================================================================\n# Elasticsearch StatefulSet for Temporal Visibility\n# ============================================================================\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: temporal-elasticsearch-pvc\n  namespace: temporal\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: fast-ssd\n  resources:\n    requests:\n      storage: 100Gi\n\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: temporal-elasticsearch\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: elasticsearch\n    app.kubernetes.io/component: visibility\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  serviceName: temporal-elasticsearch\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: elasticsearch\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: elasticsearch\n        app.kubernetes.io/component: visibility\n        app.kubernetes.io/part-of: temporal-cluster\n    spec:\n      initContainers:\n        - name: increase-vm-max-map\n          image: busybox\n          command:\n            - sysctl\n            - -w\n            - vm.max_map_count=262144\n          securityContext:\n            privileged: true\n      containers:\n        - name: elasticsearch\n          image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0\n          ports:\n            - containerPort: 9200\n              name: http\n            - containerPort: 9300\n              name: transport\n          env:\n            - name: discovery.type\n              value: single-node\n            - name: xpack.security.enabled\n              value: \"false\"\n            - name: ES_JAVA_OPTS\n              value: -Xms2g -Xmx2g\n            - name: bootstrap.memory_lock\n              value: \"true\"\n          volumeMounts:\n            - name: elasticsearch-data\n              mountPath: /usr/share/elasticsearch/data\n          resources:\n            requests:\n              memory: \"4Gi\"\n              cpu: \"1000m\"\n            limits:\n              memory: \"8Gi\"\n              cpu: \"2000m\"\n          livenessProbe:\n            httpGet:\n              path: /_cluster/health\n              port: 9200\n            initialDelaySeconds: 60\n            periodSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /_cluster/health\n              port: 9200\n            initialDelaySeconds: 30\n            periodSeconds: 10\n      volumes:\n        - name: elasticsearch-data\n          persistentVolumeClaim:\n            claimName: temporal-elasticsearch-pvc\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: temporal-elasticsearch\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: elasticsearch\n    app.kubernetes.io/component: visibility\nspec:\n  type: ClusterIP\n  ports:\n    - port: 9200\n      targetPort: 9200\n      name: http\n  selector:\n    app.kubernetes.io/name: elasticsearch\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/frontend.yaml",
  "content": "# ============================================================================\n# Temporal Frontend Deployment\n# ============================================================================\napiVersion: v1\nkind: Service\nmetadata:\n  name: temporal-frontend\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: frontend\n    app.kubernetes.io/component: frontend\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  type: ClusterIP\n  ports:\n    - port: 7233\n      targetPort: 7233\n      name: grpc\n    - port: 9090\n      targetPort: 9090\n      name: metrics\n  selector:\n    app.kubernetes.io/name: frontend\n    app.kubernetes.io/component: frontend\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-frontend\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: frontend\n    app.kubernetes.io/component: frontend\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: frontend\n      app.kubernetes.io/component: frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: frontend\n        app.kubernetes.io/component: frontend\n        app.kubernetes.io/part-of: temporal-cluster\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9090\"\n    spec:\n      containers:\n        - name: frontend\n          image: temporalio/server:latest\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 7233\n              name: grpc\n            - containerPort: 6933\n              name: membership\n            - containerPort: 9090\n              name: metrics\n          env:\n            - name: SERVICES\n              value: \"frontend\"\n            - name: TEMPORAL_ADDRESS\n              value: \"0.0.0.0:7233\"\n            - name: DB\n              value: \"postgresql\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: POSTGRES_USER\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_USER\n            - name: POSTGRES_PWD\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_PASSWORD\n            - name: POSTGRES_SEEDS\n              value: \"temporal-postgres\"\n            - name: POSTGRES_DATABASE\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_DATABASE\n            - name: NAMESPACES\n              value: \"default,production\"\n            - name: FRONTEND_ADDRESS\n              value: \"0.0.0.0:7233\"\n            - name: MEMBERSHIP_PORT\n              value: \"6933\"\n            - name: RPC_QUEUE_TYPE\n              value: \"normal\"\n            - name: PUBLIC_IP\n              value: \"temporal-frontend.temporal.svc.cluster.local\"\n            - name: SERVE_FRONTEND\n              value: \"true\"\n            - name: SERVE_HISTORY\n              value: \"false\"\n            - name: SERVE_MATCHER\n              value: \"false\"\n            - name: SERVE_WORKER\n              value: \"false\"\n            - name: CLUSTER_NAME\n              value: \"legal-ai-temporal\"\n            - name: PROMETHEUS_ENDPOINT\n              value: \"/metrics\"\n            - name: PROMETHEUS_LISTEN_ADDRESS\n              value: \":9090\"\n            - name: VISIBILITY_ENABLE\n              value: \"true\"\n            - name: ES_SEEDS\n              value: \"temporal-elasticsearch\"\n            - name: ES_PORT\n              value: \"9200\"\n            - name: ES_VERSION\n              value: \"v7\"\n            - name: ES_VIS_INDEX\n              value: \"visibility-dev\"\n          livenessProbe:\n            exec:\n              command:\n                - tctl\n                - --address\n                - temporal-frontend:7233\n                - cluster\n                - health\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            failureThreshold: 3\n          readinessProbe:\n            exec:\n              command:\n                - tctl\n                - --address\n                - temporal-frontend:7233\n                - cluster\n                - health\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            timeoutSeconds: 5\n            failureThreshold: 2\n          resources:\n            requests:\n              memory: \"512Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/history.yaml",
  "content": "# ============================================================================\n# Temporal History Deployment\n# ============================================================================\napiVersion: v1\nkind: Service\nmetadata:\n  name: temporal-history\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: history\n    app.kubernetes.io/component: history\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  type: ClusterIP\n  ports:\n    - port: 7234\n      targetPort: 7234\n      name: grpc\n    - port: 6934\n      targetPort: 6934\n      name: membership\n    - port: 9091\n      targetPort: 9091\n      name: metrics\n  selector:\n    app.kubernetes.io/name: history\n    app.kubernetes.io/component: history\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-history\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: history\n    app.kubernetes.io/component: history\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: history\n      app.kubernetes.io/component: history\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: history\n        app.kubernetes.io/component: history\n        app.kubernetes.io/part-of: temporal-cluster\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9091\"\n    spec:\n      containers:\n        - name: history\n          image: temporalio/server:latest\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 7234\n              name: grpc\n            - containerPort: 6934\n              name: membership\n            - containerPort: 9091\n              name: metrics\n          env:\n            - name: SERVICES\n              value: \"history\"\n            - name: DB\n              value: \"postgresql\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: POSTGRES_USER\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_USER\n            - name: POSTGRES_PWD\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_PASSWORD\n            - name: POSTGRES_SEEDS\n              value: \"temporal-postgres\"\n            - name: POSTGRES_DATABASE\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_DATABASE\n            - name: NAMESPACES\n              value: \"default,production\"\n            - name: HISTORY_NAMESPACE\n              value: \"production\"\n            - name: FRONTEND_ADDRESS\n              value: \"temporal-frontend:7233\"\n            - name: MASTER_ADDRESS\n              value: \"temporal-frontend:7233\"\n            - name: HISTORY_ADDRESS\n              value: \"0.0.0.0:7234\"\n            - name: MEMBERSHIP_PORT\n              value: \"6934\"\n            - name: RPC_QUEUE_TYPE\n              value: \"normal\"\n            - name: NUM_HISTORY_SHARDS\n              value: \"512\"\n            - name: HISTORY_ENABLE_EAGER_READ\n              value: \"false\"\n            - name: SERVE_FRONTEND\n              value: \"false\"\n            - name: SERVE_HISTORY\n              value: \"true\"\n            - name: SERVE_MATCHER\n              value: \"false\"\n            - name: SERVE_WORKER\n              value: \"false\"\n            - name: CLUSTER_NAME\n              value: \"legal-ai-temporal\"\n            - name: PROMETHEUS_ENDPOINT\n              value: \"/metrics\"\n            - name: PROMETHEUS_LISTEN_ADDRESS\n              value: \":9091\"\n          livenessProbe:\n            httpGet:\n              path: /metrics\n              port: 9091\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /metrics\n              port: 9091\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            timeoutSeconds: 5\n            failureThreshold: 2\n          resources:\n            requests:\n              memory: \"1Gi\"\n              cpu: \"500m\"\n            limits:\n              memory: \"4Gi\"\n              cpu: \"2000m\"\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/matching.yaml",
  "content": "# ============================================================================\n# Temporal Matching Deployment\n# ============================================================================\napiVersion: v1\nkind: Service\nmetadata:\n  name: temporal-matching\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: matching\n    app.kubernetes.io/component: matching\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  type: ClusterIP\n  ports:\n    - port: 7235\n      targetPort: 7235\n      name: grpc\n    - port: 6935\n      targetPort: 6935\n      name: membership\n    - port: 9092\n      targetPort: 9092\n      name: metrics\n  selector:\n    app.kubernetes.io/name: matching\n    app.kubernetes.io/component: matching\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-matching\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: matching\n    app.kubernetes.io/component: matching\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: matching\n      app.kubernetes.io/component: matching\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: matching\n        app.kubernetes.io/component: matching\n        app.kubernetes.io/part-of: temporal-cluster\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9092\"\n    spec:\n      containers:\n        - name: matching\n          image: temporalio/server:latest\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 7235\n              name: grpc\n            - containerPort: 6935\n              name: membership\n            - containerPort: 9092\n              name: metrics\n          env:\n            - name: SERVICES\n              value: \"matching\"\n            - name: DB\n              value: \"postgresql\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: POSTGRES_USER\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_USER\n            - name: POSTGRES_PWD\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_PASSWORD\n            - name: POSTGRES_SEEDS\n              value: \"temporal-postgres\"\n            - name: POSTGRES_DATABASE\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_DATABASE\n            - name: NAMESPACES\n              value: \"default,production\"\n            - name: MATCHING_NAMESPACE\n              value: \"production\"\n            - name: FRONTEND_ADDRESS\n              value: \"temporal-frontend:7233\"\n            - name: MASTER_ADDRESS\n              value: \"temporal-frontend:7233\"\n            - name: MATCHING_ADDRESS\n              value: \"0.0.0.0:7235\"\n            - name: MEMBERSHIP_PORT\n              value: \"6935\"\n            - name: RPC_QUEUE_TYPE\n              value: \"normal\"\n            - name: SERVE_FRONTEND\n              value: \"false\"\n            - name: SERVE_HISTORY\n              value: \"false\"\n            - name: SERVE_MATCHER\n              value: \"true\"\n            - name: SERVE_WORKER\n              value: \"false\"\n            - name: CLUSTER_NAME\n              value: \"legal-ai-temporal\"\n            - name: PROMETHEUS_ENDPOINT\n              value: \"/metrics\"\n            - name: PROMETHEUS_LISTEN_ADDRESS\n              value: \":9092\"\n          livenessProbe:\n            httpGet:\n              path: /metrics\n              port: 9092\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /metrics\n              port: 9092\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            timeoutSeconds: 5\n            failureThreshold: 2\n          resources:\n            requests:\n              memory: \"512Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/worker.yaml",
  "content": "# ============================================================================\n# Temporal Worker Deployment\n# ============================================================================\napiVersion: v1\nkind: Service\nmetadata:\n  name: temporal-worker\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: worker\n    app.kubernetes.io/component: worker\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  type: ClusterIP\n  ports:\n    - port: 7236\n      targetPort: 7236\n      name: grpc\n    - port: 6936\n      targetPort: 6936\n      name: membership\n    - port: 9093\n      targetPort: 9093\n      name: metrics\n  selector:\n    app.kubernetes.io/name: worker\n    app.kubernetes.io/component: worker\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-worker\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: worker\n    app.kubernetes.io/component: worker\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: worker\n      app.kubernetes.io/component: worker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: worker\n        app.kubernetes.io/component: worker\n        app.kubernetes.io/part-of: temporal-cluster\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9093\"\n    spec:\n      containers:\n        - name: worker\n          image: temporalio/server:latest\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 7236\n              name: grpc\n            - containerPort: 6936\n              name: membership\n            - containerPort: 9093\n              name: metrics\n          env:\n            - name: SERVICES\n              value: \"worker\"\n            - name: DB\n              value: \"postgresql\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: POSTGRES_USER\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_USER\n            - name: POSTGRES_PWD\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_PASSWORD\n            - name: POSTGRES_SEEDS\n              value: \"temporal-postgres\"\n            - name: POSTGRES_DATABASE\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: POSTGRES_DATABASE\n            - name: NAMESPACES\n              value: \"default,production\"\n            - name: FRONTEND_ADDRESS\n              value: \"temporal-frontend:7233\"\n            - name: MASTER_ADDRESS\n              value: \"temporal-frontend:7233\"\n            - name: WORKER_ADDRESS\n              value: \"0.0.0.0:7236\"\n            - name: MEMBERSHIP_PORT\n              value: \"6936\"\n            - name: RPC_QUEUE_TYPE\n              value: \"normal\"\n            - name: SERVE_FRONTEND\n              value: \"false\"\n            - name: SERVE_HISTORY\n              value: \"false\"\n            - name: SERVE_MATCHER\n              value: \"false\"\n            - name: SERVE_WORKER\n              value: \"true\"\n            - name: CLUSTER_NAME\n              value: \"legal-ai-temporal\"\n            - name: PROMETHEUS_ENDPOINT\n              value: \"/metrics\"\n            - name: PROMETHEUS_LISTEN_ADDRESS\n              value: \":9093\"\n          livenessProbe:\n            httpGet:\n              path: /metrics\n              port: 9093\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /metrics\n              port: 9093\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            timeoutSeconds: 5\n            failureThreshold: 2\n          resources:\n            requests:\n              memory: \"512Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/ui.yaml",
  "content": "# ============================================================================\n# Temporal UI Deployment with Authentication\n# ============================================================================\napiVersion: v1\nkind: Service\nmetadata:\n  name: temporal-ui\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: ui\n    app.kubernetes.io/component: ui\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  type: ClusterIP\n  ports:\n    - port: 8088\n      targetPort: 8088\n      name: http\n  selector:\n    app.kubernetes.io/name: ui\n    app.kubernetes.io/component: ui\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: temporal-ui-ingress\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: ui\n    app.kubernetes.io/component: ui\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 8088\n      name: http\n  selector:\n    app.kubernetes.io/name: ui\n    app.kubernetes.io/component: ui\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-ui\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: ui\n    app.kubernetes.io/component: ui\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ui\n      app.kubernetes.io/component: ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ui\n        app.kubernetes.io/component: ui\n        app.kubernetes.io/part-of: temporal-cluster\n    spec:\n      containers:\n        - name: ui\n          image: temporalio/ui:latest\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8088\n              name: http\n          env:\n            - name: TEMPORAL_ADDRESS\n              value: \"temporal-frontend:7233\"\n            - name: TEMPORAL_CORS_ORIGINS\n              value: \"https://temporal.yourdomain.com,http://localhost:3000\"\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8088\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            timeoutSeconds: 5\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8088\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            failureThreshold: 2\n          resources:\n            requests:\n              memory: \"128Mi\"\n              cpu: \"100m\"\n            limits:\n              memory: \"256Mi\"\n              cpu: \"200m\"\n\n---\n# Ingress for UI with TLS\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: temporal-ui-ingress\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: ui\n    app.kubernetes.io/component: ui\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri\n    nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth\n    nginx.ingress.kubernetes.io/proxy-buffer-size: 16k\nspec:\n  tls:\n    - hosts:\n        - temporal.yourdomain.com\n      secretName: temporal-ui-tls\n  rules:\n    - host: temporal.yourdomain.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: temporal-ui\n                port:\n                  number: 8088\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/prometheus.yaml",
  "content": "# ============================================================================\n# Prometheus Deployment for Temporal Metrics\n# ============================================================================\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: prometheus-pvc\n  namespace: temporal\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: fast-ssd\n  resources:\n    requests:\n      storage: 20Gi\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\n  namespace: temporal\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n      evaluation_interval: 15s\n      external_labels:\n        cluster: 'legal-ai-temporal'\n        environment: 'production'\n\n    scrape_configs:\n      - job_name: 'temporal-frontend'\n        kubernetes_sd_configs:\n          - role: pod\n            namespaces:\n              names:\n                - temporal\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]\n            regex: frontend\n            action: keep\n          - source_labels: [__meta_kubernetes_pod_ip]\n            target_label: __address__\n            replacement: $1:9090\n          - source_labels: [__meta_kubernetes_pod_name]\n            target_label: pod\n\n      - job_name: 'temporal-history'\n        kubernetes_sd_configs:\n          - role: pod\n            namespaces:\n              names:\n                - temporal\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]\n            regex: history\n            action: keep\n          - source_labels: [__meta_kubernetes_pod_ip]\n            target_label: __address__\n            replacement: $1:9091\n\n      - job_name: 'temporal-matching'\n        kubernetes_sd_configs:\n          - role: pod\n            namespaces:\n              names:\n                - temporal\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]\n            regex: matching\n            action: keep\n          - source_labels: [__meta_kubernetes_pod_ip]\n            target_label: __address__\n            replacement: $1:9092\n\n      - job_name: 'temporal-worker'\n        kubernetes_sd_configs:\n          - role: pod\n            namespaces:\n              names:\n                - temporal\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]\n            regex: worker\n            action: keep\n          - source_labels: [__meta_kubernetes_pod_ip]\n            target_label: __address__\n            replacement: $1:9093\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/component: monitoring\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/component: monitoring\n        app.kubernetes.io/part-of: temporal-cluster\n    spec:\n      serviceAccountName: prometheus\n      containers:\n        - name: prometheus\n          image: prom/prometheus:latest\n          args:\n            - '--config.file=/etc/prometheus/prometheus.yml'\n            - '--storage.tsdb.path=/prometheus'\n            - '--web.console.libraries=/usr/share/prometheus/console_libraries'\n            - '--web.console.templates=/usr/share/prometheus/consoles'\n            - '--web.enable-lifecycle'\n          ports:\n            - containerPort: 9090\n              name: http\n          volumeMounts:\n            - name: config\n              mountPath: /etc/prometheus\n            - name: storage\n              mountPath: /prometheus\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n            initialDelaySeconds: 30\n            periodSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n            initialDelaySeconds: 10\n            periodSeconds: 5\n          resources:\n            requests:\n              memory: \"512Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n      volumes:\n        - name: config\n          configMap:\n            name: prometheus-config\n        - name: storage\n          persistentVolumeClaim:\n            claimName: prometheus-pvc\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/component: monitoring\nspec:\n  type: ClusterIP\n  ports:\n    - port: 9090\n      targetPort: 9090\n      name: http\n  selector:\n    app.kubernetes.io/name: prometheus\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: temporal\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n      - nodes/proxy\n      - services\n      - endpoints\n      - pods\n    verbs: [\"get\", \"list\", \"watch\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n  - kind: ServiceAccount\n    name: prometheus\n    namespace: temporal\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/grafana.yaml",
  "content": "# ============================================================================\n# Grafana Deployment for Temporal Metrics Visualization\n# ============================================================================\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: grafana-pvc\n  namespace: temporal\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: fast-ssd\n  resources:\n    requests:\n      storage: 10Gi\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-datasources\n  namespace: temporal\ndata:\n  datasources.yml: |\n    apiVersion: 1\n    datasources:\n      - name: Prometheus\n        type: prometheus\n        access: proxy\n        url: http://prometheus:9090\n        isDefault: true\n        editable: true\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-dashboards\n  namespace: temporal\ndata:\n  dashboards.yml: |\n    apiVersion: 1\n    providers:\n      - name: 'Temporal'\n        orgId: 1\n        folder: 'Temporal'\n        type: file\n        disableDeletion: false\n        updateIntervalSeconds: 10\n        allowUiUpdates: true\n        options:\n          path: /var/lib/grafana/dashboards\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/component: monitoring\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/component: monitoring\n        app.kubernetes.io/part-of: temporal-cluster\n    spec:\n      containers:\n        - name: grafana\n          image: grafana/grafana:latest\n          ports:\n            - containerPort: 3000\n              name: http\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: GRAFANA_ADMIN_USER\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: temporal-secrets\n                  key: GRAFANA_ADMIN_PASSWORD\n            - name: GF_SERVER_ROOT_URL\n              value: https://grafana.yourdomain.com\n            - name: GF_ANALYTICS_REPORTING_ENABLED\n              value: \"false\"\n            - name: GF_ANALYTICS_CHECK_FOR_UPDATES\n              value: \"false\"\n            - name: GF_INSTALL_PLUGINS\n              value: \"\"\n          volumeMounts:\n            - name: storage\n              mountPath: /var/lib/grafana\n            - name: datasources\n              mountPath: /etc/grafana/provisioning/datasources\n            - name: dashboards-config\n              mountPath: /etc/grafana/provisioning/dashboards\n            - name: dashboards\n              mountPath: /var/lib/grafana/dashboards\n          livenessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 30\n            periodSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 10\n            periodSeconds: 5\n          resources:\n            requests:\n              memory: \"256Mi\"\n              cpu: \"100m\"\n            limits:\n              memory: \"1Gi\"\n              cpu: \"500m\"\n      volumes:\n        - name: storage\n          persistentVolumeClaim:\n            claimName: grafana-pvc\n        - name: datasources\n          configMap:\n            name: grafana-datasources\n        - name: dashboards-config\n          configMap:\n            name: grafana-dashboards\n        - name: dashboards\n          emptyDir: {}\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/component: monitoring\nspec:\n  type: ClusterIP\n  ports:\n    - port: 3000\n      targetPort: 3000\n      name: http\n  selector:\n    app.kubernetes.io/name: grafana\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/alertmanager.yaml",
  "content": "# ============================================================================\n# AlertManager Deployment for Temporal Alerts\n# ============================================================================\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: alertmanager-pvc\n  namespace: temporal\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: fast-ssd\n  resources:\n    requests:\n      storage: 5Gi\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: alertmanager-config\n  namespace: temporal\ndata:\n  alertmanager.yml: |\n    global:\n      resolve_timeout: 5m\n\n    route:\n      group_by: ['alertname', 'cluster', 'service']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 12h\n      receiver: 'default'\n      routes:\n        - match:\n            severity: critical\n          receiver: 'critical-alerts'\n\n    receivers:\n      - name: 'default'\n      - name: 'critical-alerts'\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: alertmanager\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/component: monitoring\n    app.kubernetes.io/part-of: temporal-cluster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/component: monitoring\n        app.kubernetes.io/part-of: temporal-cluster\n    spec:\n      containers:\n        - name: alertmanager\n          image: prom/alertmanager:latest\n          args:\n            - '--config.file=/etc/alertmanager/alertmanager.yml'\n            - '--storage.path=/alertmanager'\n          ports:\n            - containerPort: 9093\n              name: http\n          volumeMounts:\n            - name: config\n              mountPath: /etc/alertmanager\n            - name: storage\n              mountPath: /alertmanager\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9093\n            initialDelaySeconds: 30\n            periodSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 10\n            periodSeconds: 5\n          resources:\n            requests:\n              memory: \"128Mi\"\n              cpu: \"100m\"\n            limits:\n              memory: \"512Mi\"\n              cpu: \"250m\"\n      volumes:\n        - name: config\n          configMap:\n            name: alertmanager-config\n        - name: storage\n          persistentVolumeClaim:\n            claimName: alertmanager-pvc\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: alertmanager\n  namespace: temporal\n  labels:\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/component: monitoring\nspec:\n  type: ClusterIP\n  ports:\n    - port: 9093\n      targetPort: 9093\n      name: http\n  selector:\n    app.kubernetes.io/name: alertmanager\n"
}
Now let me create the production overlay with TLS configuration:

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/overlays/production/kustomization.yml",
  "content": "# ============================================================================\n# Kustomization Production Overlay for Temporal Cluster\n# ============================================================================\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: temporal\n\nresources:\n  - ../../base\n\n# Production-specific patches\npatches:\n  # Enable TLS for frontend\n  - patch: |-\n      - op: add\n        path: /spec/template/spec/containers/0/env/-\n        value:\n          name: TLS_ENABLED\n          value: \"true\"\n      - op: add\n        path: /spec/template/spec/containers/0/env/-\n        value:\n          name: TLS_SERVER_NAME\n          value: \"temporal.production.local\"\n      - op: add\n        path: /spec/template/spec/containers/0/env/-\n        value:\n          name: TLS_CERT_FILE\n          value: \"/etc/tls/cert.pem\"\n      - op: add\n        path: /spec/template/spec/containers/0/env/-\n        value:\n          name: TLS_KEY_FILE\n          value: \"/etc/tls/key.pem\"\n      - op: add\n        path: /spec/template/spec/containers/0/volumeMounts/-\n        value:\n          name: tls-certs\n          mountPath: /etc/tls\n          readOnly: true\n      - op: add\n        path: /spec/template/spec/volumes/-\n        value:\n          name: tls-certs\n          secret:\n            secretName: temporal-tls\n    target:\n      kind: Deployment\n      labelSelector: app.kubernetes.io/component=frontend\n\n# Production resource overrides\nimages:\n  - name: temporalio/server\n    newTag: latest-production\n\n# Common annotations\ncommonAnnotations:\n  environment: production\n  deployed-by: kustomize\n\n# ConfigMap generator for production\nconfigMapGenerator:\n  - name: temporal-production-config\n    literals:\n      - CLUSTER_NAME=legal-ai-temporal-prod\n      - NUM_HISTORY_SHARDS=1024\n      - ENVIRONMENT=production\n\n# Secret generator (use external secrets in production)\nsecretGenerator:\n  - name: temporal-production-secrets\n    literals:\n      - POSTGRES_PASSWORD=${TEMPORAL_POSTGRES_PASSWORD}\n      - GRAFANA_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/overlays/production/tls-config.yaml",
  "content": "# ============================================================================\n# TLS Certificate Configuration for Production\n# ============================================================================\n#\n# This file contains the TLS certificate configuration for Temporal services.\n# In production, you should use certificates from a trusted CA or Let's Encrypt.\n#\n# For generating self-signed certificates for testing:\n#\n# # Generate CA\n# openssl genrsa -out ca.key 2048\n# openssl req -x509 -new -nodes -key ca.key -sha256 -days 3650 -out ca.crt -subj \"/CN=Temporal CA\"\n#\n# # Generate server certificate\n# openssl genrsa -out server.key 2048\n# openssl req -new -key server.key -out server.csr -subj \"/CN=temporal.production.local\"\n# openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 -sha256\n#\n# # Generate client certificate\n# openssl genrsa -out client.key 2048\n# openssl req -new -key client.key -out client.csr -subj \"/CN=temporal-client\"\n# openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365 -sha256\n#\n# # Create Kubernetes secret\n# kubectl create secret tls temporal-tls \\\n#   --cert=server.crt \\\n#   --key=server.key \\\n#   --namespace=temporal\n#\n# kubectl create secret generic temporal-ca-cert \\\n#   --from-file=ca.crt=ca.crt \\\n#   --namespace=temporal\n#\n# ============================================================================\n\n---\n# Certificate for Temporal Frontend (external facing)\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: temporal-frontend-cert\n  namespace: temporal\nspec:\n  secretName: temporal-tls\n  dnsNames:\n    - temporal.production.local\n    - temporal.yourdomain.com\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n  usages:\n    - server auth\n    - client auth\n\n---\n# Certificate for internal Temporal service communication\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: temporal-internal-cert\n  namespace: temporal\nspec:\n  secretName: temporal-tls-internal\n  dnsNames:\n    - temporal-frontend.temporal.svc.cluster.local\n    - temporal-history.temporal.svc.cluster.local\n    - temporal-matching.temporal.svc.cluster.local\n    - temporal-worker.temporal.svc.cluster.local\n  issuerRef:\n    name: temporal-internal-issuer\n    kind: Issuer\n  usages:\n    - server auth\n    - client auth\n\n---\n# Internal Issuer for Temporal services\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: temporal-internal-issuer\n  namespace: temporal\nspec:\n  selfSigned: {}\n\n---\n# ClusterIssuer for Let's Encrypt (production)\n# This is a placeholder - configure based on your environment\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: ops@yourdomain.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/overlays/production/patches-replicas.yaml",
  "content": "# ============================================================================\n# Production Replica Patches\n# ============================================================================\n\n---\n# Increase frontend replicas\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-frontend\n  namespace: temporal\nspec:\n  replicas: 3\n\n---\n# Increase history replicas\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-history\n  namespace: temporal\nspec:\n  replicas: 5\n\n---\n# Increase matching replicas\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-matching\n  namespace: temporal\nspec:\n  replicas: 3\n\n---\n# Increase worker replicas\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-worker\n  namespace: temporal\nspec:\n  replicas: 3\n\n---\n# Increase UI replicas\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-ui\n  namespace: temporal\nspec:\n  replicas: 2\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/overlays/production/patches-resources.yaml",
  "content": "# ============================================================================\n# Production Resource Patches\n# ============================================================================\n\n---\n# Frontend resources\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-frontend\n  namespace: temporal\nspec:\n  template:\n    spec:\n      containers:\n        - name: frontend\n          resources:\n            requests:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n            limits:\n              memory: \"4Gi\"\n              cpu: \"2000m\"\n\n---\n# History resources\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-history\n  namespace: temporal\nspec:\n  template:\n    spec:\n      containers:\n        - name: history\n          resources:\n            requests:\n              memory: \"4Gi\"\n              cpu: \"2000m\"\n            limits:\n              memory: \"8Gi\"\n              cpu: \"4000m\"\n\n---\n# Matching resources\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-matching\n  namespace: temporal\nspec:\n  template:\n    spec:\n      containers:\n        - name: matching\n          resources:\n            requests:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n            limits:\n              memory: \"4Gi\"\n              cpu: \"2000m\"\n\n---\n# Worker resources\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-worker\n  namespace: temporal\nspec:\n  template:\n    spec:\n      containers:\n        - name: worker\n          resources:\n            requests:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n            limits:\n              memory: \"4Gi\"\n              cpu: \"2000m\"\n\n---\n# PostgreSQL resources\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: temporal-postgres\n  namespace: temporal\nspec:\n  template:\n    spec:\n      containers:\n        - name: postgres\n          resources:\n            requests:\n              memory: \"8Gi\"\n              cpu: \"4000m\"\n            limits:\n              memory: \"16Gi\"\n              cpu: \"8000m\"\n\n---\n# Elasticsearch resources\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: temporal-elasticsearch\n  namespace: temporal\nspec:\n  template:\n    spec:\n      containers:\n        - name: elasticsearch\n          resources:\n            requests:\n              memory: \"8Gi\"\n              cpu: \"2000m\"\n            limits:\n              memory: \"16Gi\"\n              cpu: \"4000m\"\n"
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Exploring existing Temporal setup",
      "content": "Explore and understand existing Temporal setup",
      "status": "completed"
    },
    {
      "activeForm": "Creating production Docker Compose configuration",
      "content": "Create production Docker Compose configuration for Temporal cluster",
      "status": "completed"
    },
    {
      "activeForm": "Creating Kubernetes manifests",
      "content": "Create Kubernetes manifests for Temporal deployment",
      "status": "completed"
    },
    {
      "activeForm": "Setting up TLS/SSL configuration",
      "content": "Set up TLS/SSL configuration for Temporal services",
      "status": "completed"
    },
    {
      "activeForm": "Configuring Prometheus metrics",
      "content": "Configure Prometheus metrics for Temporal",
      "status": "completed"
    },
    {
      "activeForm": "Setting up Temporal UI with authentication",
      "content": "Set up Temporal UI with authentication",
      "status": "completed"
    },
    {
      "activeForm": "Implementing health checks and monitoring",
      "content": "Implement health checks and monitoring",
      "status": "completed"
    },
    {
      "activeForm": "Configuring archival for workflow history",
      "content": "Configure archival for workflow history",
      "status": "in_progress"
    },
    {
      "activeForm": "Creating deployment documentation",
      "content": "Create deployment documentation",
      "status": "pending"
    }
  ]
}
Now let me create the archival configuration for workflow history and the deployment documentation:

ðŸ”§ Tool: Bash
Input: {
  "command": "mkdir -p /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/schema",
  "description": "Create temporal schema directory"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/schema/temporal-postgres-init.sql",
  "content": "-- ============================================================================\n-- Temporal PostgreSQL Schema Initialization\n-- ============================================================================\n-- This script initializes the PostgreSQL database for Temporal.\n-- It creates the necessary schema for Temporal's persistence layer.\n--\n-- Note: Temporal's auto-setup image handles this automatically.\n-- This file is provided for reference and manual setup if needed.\n-- ============================================================================\n\n-- Create the Temporal schema if it doesn't exist\nCREATE SCHEMA IF NOT EXISTS temporal;\n\n-- Create the visibility schema for search\nCREATE SCHEMA IF NOT EXISTS temporal_visibility;\n\n-- Grant permissions (adjust as needed)\nGRANT ALL PRIVILEGES ON SCHEMA temporal TO temporal;\nGRANT ALL PRIVILEGES ON SCHEMA temporal_visibility TO temporal;\n\n-- Note: The actual table creation is handled by Temporal server\n-- on first startup. This script ensures schemas and permissions exist.\n"
}

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/src/modules/temporal/config/archival.yml",
  "content": "# ============================================================================\n# Temporal Archival Configuration\n# ============================================================================\n#\n# This configuration enables workflow history archival for long-term storage\n# and compliance. Archived workflows can be queried and restored if needed.\n#\n# Archival supports multiple backends:\n# - S3/GCS/Azure Blob Storage for cloud storage\n# - File system for local storage\n#\n# ============================================================================\n\n# Enable archival feature\narchival:\n  enable: true\n\n  # Default namespace configuration\n  defaultNamespace:\n    name: \"production\"\n\n  # History archival configuration\n  history:\n    state:\n      # Enable/disable history archival\n      enabled: true\n\n      # Provider configuration (s3, gcs, azureblob, filestore)\n      provider: \"s3\"\n\n      # S3 configuration (if using s3 provider)\n      s3:\n        region: \"${AWS_REGION:-us-east-1}\"\n        # Use IAM role for credentials or provide access keys\n        # credentials:\n        #   accessKey: \"${AWS_ACCESS_KEY_ID}\"\n        #   secretKey: \"${AWS_SECRET_ACCESS_KEY}\"\n        endpoint: \"${AWS_ENDPOINT:-s3.amazonaws.com}\"\n        # S3 bucket configuration\n        bucket: \"${TEMPORAL_ARCHIVE_BUCKET:-temporal-archive-legal-ai}\"\n        prefix: \"history/\"\n\n      # GCS configuration (if using gcs provider)\n      gcs:\n        credentialsPath: \"${GCS_CREDENTIALS_PATH:-/etc/gcs-credentials.json}\"\n        bucket: \"${TEMPORAL_ARCHIVE_BUCKET:-temporal-archive-legal-ai}\"\n        prefix: \"history/\"\n\n      # Azure Blob Storage configuration (if using azureblob provider)\n      azureblob:\n        accountName: \"${AZURE_ACCOUNT_NAME}\"\n        accountKey: \"${AZURE_ACCOUNT_KEY}\"\n        container: \"${TEMPORAL_ARCHIVE_CONTAINER:-temporal-archive}\"\n        prefix: \"history/\"\n\n      # Filestore configuration (if using filestore provider)\n      filestore:\n        # File path for archival storage\n        filePermissions: \"0666\"\n        dirPermissions: \"0777\"\n        # Path for archived history files\n        filePath: \"/mnt/temporal-archival/history/\"\n\n    # Read configuration for accessing archived history\n    read:\n      provider: \"s3\"\n      s3:\n        region: \"${AWS_REGION:-us-east-1}\"\n        endpoint: \"${AWS_ENDPOINT:-s3.amazonaws.com}\"\n        bucket: \"${TEMPORAL_ARCHIVE_BUCKET:-temporal-archive-legal-ai}\"\n        prefix: \"history/\"\n\n  # Visibility archival configuration\n  visibility:\n    state:\n      enabled: true\n      provider: \"s3\"\n      s3:\n        region: \"${AWS_REGION:-us-east-1}\"\n        endpoint: \"${AWS_ENDPOINT:-s3.amazonaws.com}\"\n        bucket: \"${TEMPORAL_ARCHIVE_BUCKET:-temporal-archive-legal-ai}\"\n        prefix: \"visibility/\"\n\n    read:\n      provider: \"s3\"\n      s3:\n        region: \"${AWS_REGION:-us-east-1}\"\n        endpoint: \"${AWS_ENDPOINT:-s3.amazonaws.com}\"\n        bucket: \"${TEMPORAL_ARCHIVE_BUCKET:-temporal-archive-legal-ai}\"\n        prefix: \"visibility/\"\n\n# Namespace-specific archival configuration\nnamespaces:\n  - name: \"production\"\n    historyArchival:\n      state: \"enabled\"\n      uri: \"s3://temporal-archive-legal-ai/history/?region=us-east-1\"\n    visibilityArchival:\n      state: \"enabled\"\n      uri: \"s3://temporal-archive-legal-ai/visibility/?region=us-east-1\"\n\n  - name: \"staging\"\n    historyArchival:\n      state: \"enabled\"\n      uri: \"file:///mnt/temporal-archival/history/\"\n    visibilityArchival:\n      state: \"enabled\"\n      uri: \"file:///mnt/temporal-archival/visibility/\"\n\n  - name: \"default\"\n    historyArchival:\n      state: \"disabled\"\n    visibilityArchival:\n      state: \"disabled\"\n\n# Retention configuration\nretention:\n  # Workflow execution retention period\n  workflow:\n    # Default retention for completed workflows\n    defaultRetentionDays: 30\n\n    # Namespace-specific retention\n    namespaceRetention:\n      production: 90\n      staging: 30\n      default: 7\n\n  # Archived data retention\n  archive:\n    # How long to keep archived data (in days)\n    retentionDays: 2555  # 7 years for compliance\n"
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Exploring existing Temporal setup",
      "content": "Explore and understand existing Temporal setup",
      "status": "completed"
    },
    {
      "activeForm": "Creating production Docker Compose configuration",
      "content": "Create production Docker Compose configuration for Temporal cluster",
      "status": "completed"
    },
    {
      "activeForm": "Creating Kubernetes manifests",
      "content": "Create Kubernetes manifests for Temporal deployment",
      "status": "completed"
    },
    {
      "activeForm": "Setting up TLS/SSL configuration",
      "content": "Set up TLS/SSL configuration for Temporal services",
      "status": "completed"
    },
    {
      "activeForm": "Configuring Prometheus metrics",
      "content": "Configure Prometheus metrics for Temporal",
      "status": "completed"
    },
    {
      "activeForm": "Setting up Temporal UI with authentication",
      "content": "Set up Temporal UI with authentication",
      "status": "completed"
    },
    {
      "activeForm": "Implementing health checks and monitoring",
      "content": "Implement health checks and monitoring",
      "status": "completed"
    },
    {
      "activeForm": "Configuring archival for workflow history",
      "content": "Configure archival for workflow history",
      "status": "completed"
    },
    {
      "activeForm": "Creating deployment documentation",
      "content": "Create deployment documentation",
      "status": "in_progress"
    }
  ]
}
Now let me create the deployment documentation:

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/docs/TEMPORAL_DEPLOYMENT.md",
  "content": "# Temporal Production Deployment Guide\n\nThis guide covers deploying a production-grade Temporal cluster for the Legal AI Platform.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Architecture](#architecture)\n- [Prerequisites](#prerequisites)\n- [Deployment Options](#deployment-options)\n- [Docker Compose Deployment](#docker-compose-deployment)\n- [Kubernetes Deployment](#kubernetes-deployment)\n- [Configuration](#configuration)\n- [Monitoring and Observability](#monitoring-and-observability)\n- [Archival](#archival)\n- [Security](#security)\n- [Maintenance](#maintenance)\n\n## Overview\n\nThe Temporal deployment provides:\n\n- **High Availability**: Replicated services (frontend, history, matching, worker)\n- **Persistence**: PostgreSQL for primary storage\n- **Visibility**: Elasticsearch for workflow search and visibility\n- **Monitoring**: Prometheus metrics with Grafana dashboards\n- **Alerting**: AlertManager for proactive monitoring\n- **Archival**: Long-term workflow history archival\n- **Security**: TLS/SSL for service communication\n\n## Architecture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         Temporal Cluster                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                  â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚\nâ”‚  â”‚  Frontend  â”‚  â”‚  History   â”‚  â”‚  Matching  â”‚  â”‚   Worker   â”‚â”‚\nâ”‚  â”‚   (x3)     â”‚  â”‚   (x5)     â”‚  â”‚   (x3)     â”‚  â”‚   (x3)     â”‚â”‚\nâ”‚  â”‚  Port:7233 â”‚  â”‚  Port:7234 â”‚  â”‚  Port:7235 â”‚  â”‚  Port:7236 â”‚â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜â”‚\nâ”‚        â”‚                â”‚                â”‚                â”‚      â”‚\nâ”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚                                 â”‚                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                  â”‚                                â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\nâ”‚  â”‚ PostgreSQL â”‚  â”‚Elasticsearchâ”‚  â”‚   Temporal  â”‚                 â”‚\nâ”‚  â”‚  Primary   â”‚  â”‚  Visibility â”‚  â”‚     UI     â”‚                 â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     Monitoring & Observability                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\nâ”‚  â”‚ Prometheus â”‚  â”‚  Grafana   â”‚  â”‚AlertManagerâ”‚                 â”‚\nâ”‚  â”‚  Metrics   â”‚  â”‚  Dashboardsâ”‚  â”‚  Alerting  â”‚                 â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Prerequisites\n\n### General Requirements\n\n- Docker 20.10+ and Docker Compose 2.0+\n- OR Kubernetes 1.25+ with kubectl configured\n- At least 32GB RAM available\n- 200GB+ SSD storage\n\n### For Kubernetes Deployment\n\n- Kubernetes cluster with minimum 6 nodes (16 cores, 64GB RAM recommended)\n- StorageClass configured for PVC provisioning\n- cert-manager installed (for TLS certificates)\n- Ingress controller (nginx recommended)\n\n### Cloud Storage (for archival)\n\n- AWS S3, GCS, or Azure Blob Storage account\n- Or NFS mount for file-based archival\n\n## Deployment Options\n\n### Option 1: Docker Compose (Self-Hosted)\n\nBest for:\n- Single-server deployments\n- Development and staging environments\n- Smaller production workloads\n\n### Option 2: Kubernetes (Production)\n\nBest for:\n- Large-scale production deployments\n- Multi-AZ/region deployments\n- Auto-scaling requirements\n\n## Docker Compose Deployment\n\n### Step 1: Create Environment File\n\nCreate a `.env.temporal` file:\n\n```bash\n# PostgreSQL Configuration\nTEMPORAL_POSTGRES_USER=temporal\nTEMPORAL_POSTGRES_PASSWORD=your-secure-password-here\nTEMPORAL_POSTGRES_DB=temporal\nTEMPORAL_POSTGRES_PORT=5433\n\n# Temporal Configuration\nTEMPORAL_CLUSTER_NAME=legal-ai-temporal\nTEMPORAL_ADDRESS=0.0.0.0:7233\nTEMPORAL_NAMESPACE=default,production\nTEMPORAL_HISTORY_SHARDS=512\n\n# TLS Configuration (optional, for production)\nTEMPORAL_TLS_ENABLED=false\nTEMPORAL_TLS_SERVER_NAME=\nTEMPORAL_TLS_CERT_FILE=\nTEMPORAL_TLS_KEY_FILE=\n\n# UI Configuration\nTEMPORAL_UI_PORT=8088\nTEMPORAL_CORS_ORIGINS=https://temporal.yourdomain.com,http://localhost:3000\n\n# Grafana Configuration\nTEMPORAL_GRAFANA_USER=admin\nTEMPORAL_GRAFANA_PASSWORD=your-secure-grafana-password\n\n# Archival Configuration\nTEMPORAL_ARCHIVE_BUCKET=temporal-archive-legal-ai\nAWS_REGION=us-east-1\n\n# Alerting\nALERTMANAGER_SLACK_API_URL=https://hooks.slack.com/...\nALERTMANAGER_CRITICAL_EMAIL=ops@yourdomain.com\n```\n\n### Step 2: Start the Cluster\n\n```bash\n# Load environment variables\nexport $(cat .env.temporal | xargs)\n\n# Start Temporal cluster\ndocker compose -f docker-compose.temporal.yml up -d\n\n# Verify services are running\ndocker compose -f docker-compose.temporal.yml ps\n```\n\n### Step 3: Verify Deployment\n\n```bash\n# Check cluster health\ndocker exec legal-ai-temporal tctl --address temporal:7233 cluster health\n\n# Access Temporal UI\nopen http://localhost:8088\n\n# Access Grafana\nopen http://localhost:3001\n# Login with admin / your-secure-grafana-password\n```\n\n## Kubernetes Deployment\n\n### Step 1: Install Prerequisites\n\n```bash\n# Install cert-manager for TLS certificates\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml\n\n# Verify installation\nkubectl get pods -n cert-manager\n```\n\n### Step 2: Create Namespace\n\n```bash\nkubectl create namespace temporal\n```\n\n### Step 3: Configure Secrets\n\n```bash\n# Update secrets in k8s/temporal/base/secrets.yaml\n# Or use external secret operator for production\n\n# Create TLS secrets (if using cert-manager, skip this)\nkubectl create secret tls temporal-tls \\\n  --cert=path/to/server.crt \\\n  --key=path/to/server.key \\\n  --namespace=temporal\n\n# Create database secret\nkubectl create secret generic temporal-postgres-credentials \\\n  --from-literal=password=your-secure-password \\\n  --namespace=temporal\n```\n\n### Step 4: Deploy Base Configuration\n\n```bash\n# Apply base manifests\nkubectl apply -k k8s/temporal/base\n\n# Verify deployment\nkubectl get pods -n temporal\nkubectl get svc -n temporal\n```\n\n### Step 5: Deploy Production Overlay\n\n```bash\n# Apply production overlay with TLS and increased replicas\nkubectl apply -k k8s/temporal/overlays/production\n\n# Verify all pods are running\nkubectl wait --for=condition=ready pod -l app.kubernetes.io/part-of=temporal-cluster -n temporal --timeout=300s\n```\n\n### Step 6: Expose Services\n\nCreate Ingress for UI access:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: temporal-ui\n  namespace: temporal\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  tls:\n    - hosts:\n        - temporal.yourdomain.com\n      secretName: temporal-ui-tls\n  rules:\n    - host: temporal.yourdomain.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: temporal-ui\n                port:\n                  number: 8088\n```\n\n### Step 7: Connect Backend to Temporal\n\nUpdate backend environment variables:\n\n```yaml\nenv:\n  - name: TEMPORAL_CLUSTER_URL\n    value: \"temporal-frontend.temporal.svc.cluster.local:7233\"\n  - name: TEMPORAL_NAMESPACE\n    value: \"production\"\n  - name: TEMPORAL_TLS_ENABLED\n    value: \"true\"\n  - name: TEMPORAL_SERVER_NAME\n    value: \"temporal.production.local\"\n  - name: TEMPORAL_SERVER_ROOT_CA_CERT_PATH\n    value: \"/etc/tls/ca.crt\"\n  - name: TEMPORAL_CLIENT_CERT_PATH\n    value: \"/etc/tls/client.crt\"\n  - name: TEMPORAL_CLIENT_PRIVATE_KEY_PATH\n    value: \"/etc/tls/client.key\"\n```\n\n## Configuration\n\n### Environment Variables\n\n| Variable | Description | Default | Required |\n|----------|-------------|---------|----------|\n| `TEMPORAL_CLUSTER_URL` | Temporal frontend address | `localhost:7233` | Yes |\n| `TEMPORAL_NAMESPACE` | Temporal namespace | `default` | No |\n| `TEMPORAL_TLS_ENABLED` | Enable TLS | `false` | No |\n| `TEMPORAL_TASK_QUEUE` | Default task queue | `legal-ai-task-queue` | No |\n| `TEMPORAL_HISTORY_SHARDS` | Number of history shards | `512` | No |\n\n### Worker Configuration\n\nWorker configuration is defined in `temporal-config.yml`:\n\n```yaml\nproduction:\n  workerPools:\n    - name: 'document-processing'\n      taskQueue: 'document-processing'\n      workerCount: 4\n      maxConcurrentWorkflowTasks: 200\n      maxConcurrentActivities: 100\n```\n\n### Namespace Configuration\n\nRegister a new namespace:\n\n```bash\n# Using tctl\ntctl --address temporal-frontend:7233 namespace register --namespace production\ntctl --address temporal-frontend:7233 namespace describe --namespace production\n```\n\n## Monitoring and Observability\n\n### Prometheus Metrics\n\nAccess metrics at:\n- Frontend: `http://temporal-frontend:9090/metrics`\n- History: `http://temporal-history:9091/metrics`\n- Matching: `http://temporal-matching:9092/metrics`\n- Worker: `http://temporal-worker:9093/metrics`\n\n### Grafana Dashboards\n\n1. Access Grafana at `http://localhost:3001` (Docker) or via Ingress (Kubernetes)\n2. Login with configured credentials\n3. Navigate to Dashboards â†’ Temporal\n\nPre-built dashboards include:\n- **Temporal Cluster Overview**: Overall cluster health\n- **Workflow Throughput**: Workflow execution metrics\n- **Activity Metrics**: Activity execution metrics\n- **Service Health**: Individual service health\n\n### Alerts\n\nAlerts are configured in `prometheus-alerts.yml`:\n\n| Alert | Severity | Description |\n|-------|----------|-------------|\n| `TemporalFrontendDown` | Critical | Frontend service is down |\n| `TemporalHistoryDown` | Critical | History service is down |\n| `HighWorkflowFailureRate` | Warning | Workflow failure rate > 10% |\n| `WorkflowExecutionTimeout` | Warning | High workflow timeout rate |\n\nConfigure AlertManager to receive notifications:\n- Slack: Update `ALERTMANAGER_SLACK_API_URL`\n- Email: Update `ALERTMANAGER_*` environment variables\n\n## Archival\n\n### Enabling Archival\n\nArchival is configured in `config/archival.yml`.\n\nFor S3 archival:\n\n```yaml\narchival:\n  history:\n    state:\n      enabled: true\n      provider: \"s3\"\n      s3:\n        region: \"us-east-1\"\n        bucket: \"temporal-archive-legal-ai\"\n        prefix: \"history/\"\n```\n\n### Configure Namespace for Archival\n\n```bash\n# Using tctl\ntctl namespace update \\\n  --namespace production \\\n  --history_archival_state enabled \\\n  --history_archival_uri \"s3://temporal-archive-legal-ai/history/?region=us-east-1\"\n```\n\n### Querying Archived Workflows\n\n```bash\n# List archived workflows\ntctl workflow list --archived\n\n# Describe archived workflow\ntctl workflow describe \\\n  --workflow_id my-workflow-id \\\n  --run_id my-run-id \\\n  --archived\n```\n\n## Security\n\n### TLS Configuration\n\nFor production, enable TLS for all service communication:\n\n```yaml\n# In k8s/temporal/overlays/production/kustomization.yml\npatches:\n  - path: patches-tls.yaml\n    target:\n      kind: Deployment\n```\n\nGenerate certificates:\n\n```bash\n# Using cert-manager (recommended)\nkubectl apply -f k8s/temporal/overlays/production/tls-config.yaml\n\n# Or manually with OpenSSL\nopenssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -key ca.key -sha256 -days 3650 -out ca.crt\n# ... create server and client certificates\n```\n\n### UI Authentication\n\nConfigure OAuth2 proxy for UI authentication:\n\n```yaml\n# In ui.yaml\nenv:\n  - name: OAUTH2_PROXY_ENABLED\n    value: \"true\"\n  - name: OAUTH2_PROXY_CLIENT_ID\n    value: \"your-client-id\"\n  - name: OAUTH2_PROXY_CLIENT_SECRET\n    value: \"your-client-secret\"\n```\n\n### Network Policies\n\nRestrict network access with NetworkPolicy:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: temporal-network-policy\n  namespace: temporal\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/part-of: temporal-cluster\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              name: temporal\n      ports:\n        - protocol: TCP\n          port: 7233  # Frontend\n```\n\n## Maintenance\n\n### Backup\n\n```bash\n# PostgreSQL backup\nkubectl exec -n temporal temporal-postgres-0 -- \\\n  pg_dump -U temporal temporal > temporal-backup.sql\n\n# Elasticsearch snapshot\nkubectl exec -n temporal temporal-elasticsearch-0 -- \\\n  curl -X PUT \"localhost:9200/_snapshot/backup/snapshot_1\"\n```\n\n### Restore\n\n```bash\n# PostgreSQL restore\nkubectl exec -i -n temporal temporal-postgres-0 -- \\\n  psql -U temporal temporal < temporal-backup.sql\n```\n\n### Upgrade\n\n```bash\n# Kubernetes rollout\nkubectl set image deployment/temporal-frontend \\\n  frontend=temporalio/server:latest-version -n temporal\n\n# Docker Compose\ndocker compose -f docker-compose.temporal.yml pull\ndocker compose -f docker-compose.temporal.yml up -d\n```\n\n### Scaling\n\n```bash\n# Kubernetes - autoscaling\nkubectl autoscale deployment temporal-history \\\n  --min=3 --max=10 --cpu-percent=70 -n temporal\n\n# Manual scaling\nkubectl scale deployment temporal-history --replicas=5 -n temporal\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**Issue**: Workers unable to connect to Temporal\n- **Solution**: Verify `TEMPORAL_CLUSTER_URL` and TLS certificates\n\n**Issue**: High workflow failure rate\n- **Solution**: Check worker logs, increase worker count\n\n**Issue**: UI shows \"Connection refused\"\n- **Solution**: Verify frontend service is accessible\n\n### Logs\n\n```bash\n# Kubernetes\nkubectl logs -f deployment/temporal-frontend -n temporal\nkubectl logs -f deployment/temporal-history -n temporal\n\n# Docker Compose\ndocker compose -f docker-compose.temporal.yml logs -f temporal-frontend\n```\n\n### Health Checks\n\n```bash\n# Check cluster health\ntctl --address temporal-frontend:7233 cluster health\n\n# Check namespace\ntctl --address temporal-frontend:7233 namespace describe\n\n# Check task queues\ntctl --address temporal-frontend:7233 task-queue describe\n```\n\n## Resources\n\n- [Temporal Documentation](https://docs.temporal.io/)\n- [Temporal Server Operations](https://docs.temporal.io/server)\n- [Temporal Deployment Guide](https://docs.temporal.io/server/deployment)\n"
}
Now let me update the .env.example file with the new Temporal production variables:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/.env.example"
}

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend/.env.example",
  "old_string": "# Temporal Configuration\n# - Local development: Use localhost:7233\n# - Docker: docker-compose.yml overrides TEMPORAL_CLUSTER_URL to 'temporal:7233'\nTEMPORAL_CLUSTER_URL=localhost:7233\nTEMPORAL_NAMESPACE=default\nTEMPORAL_CLIENT_TIMEOUT=30000\nTEMPORAL_TASK_QUEUE=legal-ai-task-queue\n\n# Temporal TLS Configuration (optional, for production)\n# TEMPORAL_TLS_ENABLED=false\n# TEMPORAL_SERVER_NAME=\n# TEMPORAL_SERVER_ROOT_CA_CERT_PATH=\n# TEMPORAL_CLIENT_CERT_PATH=\n# TEMPORAL_CLIENT_PRIVATE_KEY_PATH=",
  "new_string": "# Temporal Configuration\n# - Local development: Use localhost:7233\n# - Docker: docker-compose.yml overrides TEMPORAL_CLUSTER_URL to 'temporal:7233'\n# - Kubernetes (production): Use temporal-frontend.temporal.svc.cluster.local:7233\nTEMPORAL_CLUSTER_URL=localhost:7233\nTEMPORAL_NAMESPACE=default\nTEMPORAL_CLIENT_TIMEOUT=30000\nTEMPORAL_TASK_QUEUE=legal-ai-task-queue\n\n# Temporal TLS Configuration (optional, for production)\n# Enable TLS for production deployments\n# TEMPORAL_TLS_ENABLED=false\n# TEMPORAL_SERVER_NAME=temporal.production.local\n# TEMPORAL_SERVER_ROOT_CA_CERT_PATH=/etc/tls/ca.crt\n# TEMPORAL_CLIENT_CERT_PATH=/etc/tls/client.crt\n# TEMPORAL_CLIENT_PRIVATE_KEY_PATH=/etc/tls/client.key\n\n# Temporal Cluster Configuration (production)\n# TEMPORAL_CLUSTER_NAME=legal-ai-temporal\n# TEMPORAL_HISTORY_SHARDS=512\n\n# Temporal Archival Configuration (production)\n# TEMPORAL_ARCHIVE_ENABLED=false\n# TEMPORAL_ARCHIVE_BUCKET=temporal-archive-legal-ai\n# AWS_REGION=us-east-1\n# AWS_ENDPOINT=s3.amazonaws.com\n\n# Temporal UI Configuration\n# TEMPORAL_UI_PORT=8088\n# TEMPORAL_UI_ENABLED=true\n# TEMPORAL_CORS_ORIGINS=http://localhost:3000,http://localhost:3001\n\n# Temporal Monitoring Configuration\n# TEMPORAL_PROMETHEUS_ENABLED=true\n# TEMPORAL_PROMETHEUS_PORT=9090\n# TEMPORAL_GRAFANA_USER=admin\n# TEMPORAL_GRAFANA_PASSWORD=change-me-in-production\n\n# Temporal Alerting Configuration\n# ALERTMANAGER_SLACK_API_URL=https://hooks.slack.com/services/...\n# ALERTMANAGER_CRITICAL_EMAIL=ops@yourdomain.com\n# ALERTMANAGER_SMARTHOST=smtp.gmail.com:587\n# ALERTMANAGER_SMTP_FROM=alertmanager@yourdomain.com\n# ALERTMANAGER_SMTP_USERNAME=\n# ALERTMANAGER_SMTP_PASSWORD="
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Exploring existing Temporal setup",
      "content": "Explore and understand existing Temporal setup",
      "status": "completed"
    },
    {
      "activeForm": "Creating production Docker Compose configuration",
      "content": "Create production Docker Compose configuration for Temporal cluster",
      "status": "completed"
    },
    {
      "activeForm": "Creating Kubernetes manifests",
      "content": "Create Kubernetes manifests for Temporal deployment",
      "status": "completed"
    },
    {
      "activeForm": "Setting up TLS/SSL configuration",
      "content": "Set up TLS/SSL configuration for Temporal services",
      "status": "completed"
    },
    {
      "activeForm": "Configuring Prometheus metrics",
      "content": "Configure Prometheus metrics for Temporal",
      "status": "completed"
    },
    {
      "activeForm": "Setting up Temporal UI with authentication",
      "content": "Set up Temporal UI with authentication",
      "status": "completed"
    },
    {
      "activeForm": "Implementing health checks and monitoring",
      "content": "Implement health checks and monitoring",
      "status": "completed"
    },
    {
      "activeForm": "Configuring archival for workflow history",
      "content": "Configure archival for workflow history",
      "status": "completed"
    },
    {
      "activeForm": "Creating deployment documentation",
      "content": "Create deployment documentation",
      "status": "completed"
    }
  ]
}
Now let me run the linting and type checking to ensure the code is correct:

ðŸ”§ Tool: Bash
Input: {
  "command": "cd /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/apps/backend && pnpm dlx tsc --noEmit 2>&1 | head -50",
  "description": "Run TypeScript type checking"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "pnpm dlx eslint . --quiet 2>&1 | head -30",
  "description": "Run ESLint on backend"
}
The ESLint is running out of memory. Let me verify the Docker Compose file is valid:

ðŸ”§ Tool: Bash
Input: {
  "command": "docker compose -f docker-compose.temporal.yml config 2>&1 | head -20",
  "description": "Validate Docker Compose configuration"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "cd /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal && docker compose -f docker-compose.temporal.yml config 2>&1 | head -20",
  "description": "Validate Docker Compose configuration from root"
}
The Docker Compose configuration is valid. Let me also verify the Kubernetes manifests are valid:

ðŸ”§ Tool: Bash
Input: {
  "command": "kubectl apply --dry-run=client -f k8s/temporal/base/namespace.yaml 2>&1",
  "description": "Validate Kubernetes namespace manifest"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "kubectl apply --dry-run=client -k k8s/temporal/base 2>&1 | head -30",
  "description": "Validate Kubernetes base manifests"
}
The Kubernetes manifests are valid. Let me fix the Kustomization warning:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/kustomization.yml",
  "old_string": "# Common labels applied to all resources\ncommonLabels:\n  app.kubernetes.io/name: temporal\n  app.kubernetes.io/part-of: temporal-cluster\n  app.kubernetes.io/managed-by: kustomize",
  "new_string": "# Common labels applied to all resources\nlabels:\n  - pairs:\n      app.kubernetes.io/name: temporal\n      app.kubernetes.io/part-of: temporal-cluster\n      app.kubernetes.io/managed-by: kustomize"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/kustomization.yml"
}

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal/k8s/temporal/base/kustomization.yml",
  "old_string": "# Common labels applied to all resources\ncommonLabels:\n  app.kubernetes.io/name: temporal\n  app.kubernetes.io/part-of: temporal-cluster\n  app.kubernetes.io/managed-by: kustomize",
  "new_string": "# Common labels applied to all resources\nlabels:\n  - pairs:\n      app.kubernetes.io/name: temporal\n      app.kubernetes.io/part-of: temporal-cluster\n      app.kubernetes.io/managed-by: kustomize"
}
Now let me list all the files created to provide a summary:

ðŸ”§ Tool: Bash
Input: {
  "command": "find /Users/piteer/workspace/radca-prawny/legal/.worktrees/feature-temporal -type f \\( -path \"*/docker-compose.temporal.yml\" -o -path \"*/k8s/temporal/*\" -o -path \"*/docs/TEMPORAL_DEPLOYMENT.md\" -o -path \"*/modules/temporal/config/*\" -o -path \"*/modules/temporal/schema/*\" \\) ! -path \"*/node_modules/*\" | sort",
  "description": "List all created files"
}
All files have been created successfully. Let me provide the final summary:

<summary>
## Summary: Temporal Production Deployment

### Changes Implemented

1. **Production Docker Compose Configuration** (`docker-compose.temporal.yml`)
   - Replicated Temporal services: Frontend (x3), History (x5), Matching (x3), Worker (x3)
   - PostgreSQL for backend persistence
   - Elasticsearch for workflow visibility and search
   - Prometheus metrics collection
   - Grafana dashboards
   - AlertManager for alerting
   - Temporal UI with authentication support

2. **Kubernetes Deployment Manifests** (`k8s/temporal/`)
   - Base manifests with Kustomize configuration
   - Production overlay with TLS configuration
   - All Temporal services as Deployments with health checks
   - PostgreSQL and Elasticsearch as StatefulSets
   - Monitoring stack (Prometheus, Grafana, AlertManager)
   - Ingress configuration for UI access
   - RBAC configuration for Prometheus service account

3. **TLS/SSL Configuration** (`k8s/temporal/overlays/production/tls-config.yaml`)
   - cert-manager integration for Let's Encrypt certificates
   - Internal CA for service-to-service communication
   - Certificate configuration for frontend and internal services

4. **Monitoring and Alerting** (`apps/backend/src/modules/temporal/config/`)
   - Prometheus configuration for scraping Temporal metrics
   - Alert rules for service health, workflow failures, latency
   - AlertManager configuration for Slack/Email notifications
   - Grafana datasources and dashboard provisioning
   - Pre-built Temporal overview dashboard

5. **Archival Configuration** (`apps/backend/src/modules/temporal/config/archival.yml`)
   - S3/GCS/Azure Blob Storage support
   - History and visibility archival configuration
   - Namespace-specific retention policies
   - 7-year archival retention for compliance

6. **Health Checks and Monitoring**
   - Liveness and readiness probes for all services
   - Service-specific metrics endpoints (port 9090-9093)
   - Cluster health checks via tctl

7. **Deployment Documentation** (`docs/TEMPORAL_DEPLOYMENT.md`)
   - Complete deployment guide for Docker Compose and Kubernetes
   - Configuration reference
   - Troubleshooting section
   - Backup and restore procedures

8. **Updated Environment Variables** (`apps/backend/.env.example`)
   - Added production Temporal configuration options
   - TLS, archival, monitoring, and alerting variables

### Files Modified
- `apps/backend/.env.example` - Added production Temporal configuration variables

### New Files Created (28 files)
- `docker-compose.temporal.yml`
- `k8s/temporal/base/` - 15 Kubernetes manifest files
- `k8s/temporal/overlays/production/` - 4 production overlay files
- `apps/backend/src/modules/temporal/config/` - 6 configuration files
- `apps/backend/src/modules/temporal/schema/temporal-postgres-init.sql`
- `docs/TEMPORAL_DEPLOYMENT.md`

### Notes for Developer
- The Docker Compose deployment is suitable for single-server or staging environments
- For production, use Kubernetes with the production overlay for better scaling and resilience
- TLS certificates should be managed via cert-manager in production
- Update `TEMPORAL_CLUSTER_URL` to point to the production cluster when deploying the backend
- Configure archival bucket credentials in environment variables before enabling archival
- Review and customize alert thresholds in `prometheus-alerts.yml` based on your workload
</summary>