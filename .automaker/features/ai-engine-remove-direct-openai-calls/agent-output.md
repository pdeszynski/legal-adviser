I'll start by exploring the codebase to understand the current OpenAI usage in the AI Engine.

ðŸ”§ Tool: Task
Input: {
  "description": "Explore OpenAI usage in AI Engine",
  "prompt": "Search the AI Engine codebase for all direct OpenAI API usage. Look for:\n\n1. Imports of 'openai', 'openai.AsyncOpenAI', 'from openai import ...'\n2. Direct client calls like 'client.chat.completions.create()'\n3. Any files that use the OpenAI client directly\n\nSearch in apps/ai-engine directory. Report all findings with file paths and line numbers.",
  "subagent_type": "Explore"
}

ðŸ”§ Tool: Grep
Input: {
  "path": "apps/ai-engine",
  "pattern": "import.*openai",
  "output_mode": "content",
  "-n": true,
  "type": "py"
}

ðŸ”§ Tool: Grep
Input: {
  "path": "apps/ai-engine",
  "pattern": "AsyncOpenAI|openai\\.(Async)?OpenAI",
  "output_mode": "content",
  "-n": true,
  "type": "py"
}

ðŸ”§ Tool: Grep
Input: {
  "path": "apps/ai-engine",
  "pattern": "client\\.(chat|embeddings)\\.",
  "output_mode": "content",
  "-n": true,
  "type": "py"
}

ðŸ”§ Tool: Grep
Input: {
  "path": "apps/ai-engine",
  "pattern": "\\.completions\\.create\\(|\\.embeddings\\.create\\(",
  "output_mode": "content",
  "-n": true,
  "type": "py"
}

ðŸ”§ Tool: Grep
Input: {
  "path": "apps/ai-engine",
  "pattern": "openai\\.(chat|embeddings)\\.",
  "output_mode": "content",
  "-n": true,
  "type": "py"
}

ðŸ”§ Tool: Grep
Input: {
  "path": "apps/ai-engine",
  "pattern": "import openai",
  "output_mode": "content",
  "-n": true,
  "type": "py"
}

ðŸ”§ Tool: Glob
Input: {
  "path": "apps/ai-engine",
  "pattern": "**/__init__.py"
}

ðŸ”§ Tool: Grep
Input: {
  "path": "apps/ai-engine/src",
  "pattern": "openai",
  "output_mode": "content",
  "-n": true,
  "type": "py"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/services/embedding_service.py",
  "limit": 15,
  "offset": 35
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/qa_agent.py",
  "limit": 10,
  "offset": 105
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/sentry_init.py",
  "limit": 20,
  "offset": 185
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/sentry_init.py",
  "limit": 30
}

ðŸ”§ Tool: Grep
Input: {
  "path": "apps/ai-engine/src/sentry_init.py",
  "pattern": "import openai",
  "output_mode": "content",
  "-n": true
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/sentry_init.py",
  "limit": 30,
  "offset": 190
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/dependencies.py",
  "limit": 50
}
Let me read the key files to understand the current implementation and plan the migration:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/dependencies.py"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/services/embedding_service.py"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/qa_agent.py"
}
Now let me check the config.py to understand the settings and look at how PydanticAI handles embeddings:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/config.py"
}

ðŸ”§ Tool: Grep
Input: {
  "pattern": "embedding|Embedding",
  "path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine",
  "output_mode": "content",
  "-n": true
}
Let me check the main.py to see how embeddings are used:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/main.py",
  "offset": 160,
  "limit": 100
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/main.py",
  "offset": 500,
  "limit": 50
}
Now I have a complete picture. Let me check if PydanticAI has built-in embedding support or if we need to use a different approach:

ðŸ”§ Tool: WebSearch
Input: {
  "query": "pydantic-ai embeddings openai text-embedding-3-small 2025"
}
**ðŸŒ Z.ai Built-in Tool: webReader**

**Input:**
```json
{"url":"https://ai.pydantic.dev/embeddings/","return_format":"markdown"}
```

*Executing on server...*
                                            **Output:**
**webReader_result_summary:** [{"text": {"title": "Embeddings - Pydantic AI", "description": "GenAI Agent Framework, the Pydantic way", "url": "https://ai.pydantic.dev/embeddings/", "content": "Embeddings are vector representations of text that capture semantic meaning. They're essential for building:\n\n- __Semantic search__ â€” ...
                                                Now I understand the full picture. PydanticAI has built-in `Embedder` support that we should use instead of direct OpenAI client calls. Let me create a todo list and implement the changes:

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "content": "Refactor dependencies.py to remove direct OpenAI import and use PydanticAI Embedder",
      "status": "in_progress",
      "activeForm": "Refactoring dependencies.py to use PydanticAI Embedder"
    },
    {
      "content": "Refactor embedding_service.py to use PydanticAI Embedder instead of direct OpenAI client",
      "status": "pending",
      "activeForm": "Refactoring embedding_service.py to use PydanticAI Embedder"
    },
    {
      "content": "Refactor qa_agent.py to use PydanticAI Embedder for query embeddings",
      "status": "pending",
      "activeForm": "Refactoring qa_agent.py to use PydanticAI Embedder"
    },
    {
      "content": "Update tests to work with refactored code",
      "status": "pending",
      "activeForm": "Updating tests"
    },
    {
      "content": "Run tests to verify changes",
      "status": "pending",
      "activeForm": "Running tests"
    }
  ]
}
Now I have all the information I need. PydanticAI has built-in `Embedder` support. Let me refactor the code to use PydanticAI's `Embedder` instead of direct OpenAI client calls:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/dependencies.py",
  "old_string": "\"\"\"PydanticAI dependency injection system for LLM models and services.\n\nThis module provides centralized dependency management for PydanticAI agents,\nfollowing PydanticAI's dependency injection patterns.\n\"\"\"\n\nfrom functools import lru_cache\n\nfrom openai import AsyncOpenAI\nfrom pydantic_ai import Agent\n\nfrom ..config import get_settings\n\n\n@lru_cache\ndef get_openai_client() -> AsyncOpenAI:\n    \"\"\"Get or create the OpenAI client singleton.\n\n    This client is used for direct OpenAI API calls (e.g., embeddings).\n    For agent-based interactions, use the model dependency instead.\n    \"\"\"\n    settings = get_settings()\n    return AsyncOpenAI(api_key=settings.OPENAI_API_KEY)\n\n\nclass ModelDeps:\n    \"\"\"Dependency container for model-related dependencies.\n\n    This class can be passed to PydanticAI agents to provide\n    access to models and clients.\n\n    Example:\n        ```python\n        agent = Agent(\n            \"openai:gpt-4o\",\n            deps_type=ModelDeps,\n        )\n\n        @agent.system_prompt\n        async def system_prompt(ctx: RunContext[ModelDeps]) -> str:\n            return \"You are a helpful legal assistant.\"\n        ```\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize dependencies with cached models and clients.\"\"\"\n        self.settings = get_settings()\n        self.openai_client = get_openai_client()",
  "new_string": "\"\"\"PydanticAI dependency injection system for LLM models and services.\n\nThis module provides centralized dependency management for PydanticAI agents,\nfollowing PydanticAI's dependency injection patterns.\n\"\"\"\n\nfrom functools import lru_cache\n\nfrom pydantic_ai import Agent, Embedder\n\nfrom ..config import get_settings\n\n\n@lru_cache\ndef get_embedding_embedder() -> Embedder:\n    \"\"\"Get or create the PydanticAI Embedder singleton for embeddings.\n\n    This embedder is used for generating text embeddings via PydanticAI's\n    unified embedding interface. It abstracts away direct OpenAI API calls.\n    \"\"\"\n    settings = get_settings()\n    return Embedder(f\"openai:{settings.OPENAI_EMBEDDING_MODEL}\")\n\n\nclass ModelDeps:\n    \"\"\"Dependency container for model-related dependencies.\n\n    This class can be passed to PydanticAI agents to provide\n    access to models and embedders.\n\n    Example:\n        ```python\n        agent = Agent(\n            \"openai:gpt-4o\",\n            deps_type=ModelDeps,\n        )\n\n        @agent.system_prompt\n        async def system_prompt(ctx: RunContext[ModelDeps]) -> str:\n            return \"You are a helpful legal assistant.\"\n        ```\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize dependencies with cached models and embedders.\"\"\"\n        self.settings = get_settings()\n        self.embedding_embedder = get_embedding_embedder()"
}
Now let me add the OPENAI_EMBEDDING_MODEL setting to config.py:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/config.py",
  "old_string": "    # OpenAI Settings\n    # Default placeholder allows service startup;\n    # actual AI features will fail without a real key.\n    OPENAI_API_KEY: str = \"sk-placeholder-set-real-key-in-env\"\n    OPENAI_MODEL: str = \"gpt-4o\"",
  "new_string": "    # OpenAI Settings\n    # Default placeholder allows service startup;\n    # actual AI features will fail without a real key.\n    OPENAI_API_KEY: str = \"sk-placeholder-set-real-key-in-env\"\n    OPENAI_MODEL: str = \"gpt-4o\"\n    OPENAI_EMBEDDING_MODEL: str = \"text-embedding-3-small\""
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Refactoring dependencies.py to use PydanticAI Embedder",
      "content": "Refactor dependencies.py to remove direct OpenAI import and use PydanticAI Embedder",
      "status": "completed"
    },
    {
      "activeForm": "Refactoring embedding_service.py to use PydanticAI Embedder",
      "content": "Refactor embedding_service.py to use PydanticAI Embedder instead of direct OpenAI client",
      "status": "in_progress"
    },
    {
      "activeForm": "Refactoring qa_agent.py to use PydanticAI Embedder",
      "content": "Refactor qa_agent.py to use PydanticAI Embedder for query embeddings",
      "status": "pending"
    },
    {
      "activeForm": "Updating tests",
      "content": "Update tests to work with refactored code",
      "status": "pending"
    },
    {
      "activeForm": "Running tests",
      "content": "Run tests to verify changes",
      "status": "pending"
    }
  ]
}
Now let me refactor embedding_service.py to use PydanticAI's Embedder:

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/services/embedding_service.py",
  "content": "\"\"\"Embedding generation service for vector store.\n\nThis service handles text embedding generation using PydanticAI's Embedder,\nwhich provides a unified interface for multiple embedding providers.\n\"\"\"\n\nfrom ..agents.dependencies import get_embedding_embedder\n\n\nclass EmbeddingService:\n    \"\"\"Service for generating text embeddings using PydanticAI.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the embedding service with shared PydanticAI Embedder.\"\"\"\n        self.embedder = get_embedding_embedder()\n\n    async def generate_embeddings(\n        self, texts: list[str], model: str | None = None\n    ) -> list[list[float]]:\n        \"\"\"Generate embeddings for a list of texts.\n\n        Args:\n            texts: List of text strings to embed\n            model: Embedding model to use (overrides the default from config)\n\n        Returns:\n            List of embedding vectors (one per input text)\n\n        Raises:\n            Exception: If embedding generation fails\n        \"\"\"\n        if not texts:\n            return []\n\n        try:\n            # Use the embedder's embed_documents method for content indexing\n            result = await self.embedder.embed_documents(texts)\n\n            # Extract embeddings from result\n            return [list(emb) for emb in result.embeddings]\n\n        except Exception as e:\n            raise RuntimeError(f\"Embedding generation failed: {e!s}\") from e\n\n    async def generate_embedding(\n        self, text: str, model: str | None = None\n    ) -> list[float]:\n        \"\"\"Generate embedding for a single text.\n\n        Args:\n            text: Text string to embed\n            model: Embedding model to use (overrides the default from config)\n\n        Returns:\n            Single embedding vector\n\n        Raises:\n            Exception: If embedding generation fails\n        \"\"\"\n        embeddings = await self.generate_embeddings([text], model)\n        return embeddings[0] if embeddings else []\n"
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Refactoring dependencies.py to use PydanticAI Embedder",
      "content": "Refactor dependencies.py to remove direct OpenAI import and use PydanticAI Embedder",
      "status": "completed"
    },
    {
      "activeForm": "Refactoring embedding_service.py to use PydanticAI Embedder",
      "content": "Refactor embedding_service.py to use PydanticAI Embedder instead of direct OpenAI client",
      "status": "completed"
    },
    {
      "activeForm": "Refactoring qa_agent.py to use PydanticAI Embedder",
      "content": "Refactor qa_agent.py to use PydanticAI Embedder for query embeddings",
      "status": "in_progress"
    },
    {
      "activeForm": "Updating tests",
      "content": "Update tests to work with refactored code",
      "status": "pending"
    },
    {
      "activeForm": "Running tests",
      "content": "Run tests to verify changes",
      "status": "pending"
    }
  ]
}
Now let me refactor qa_agent.py to use the PydanticAI Embedder:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/qa_agent.py",
  "old_string": "from ..config import get_settings\nfrom ..langfuse_init import _redact_dict_pii, is_langfuse_enabled\nfrom .clarification_agent import generate_clarifications\nfrom .dependencies import ModelDeps, get_model_deps, get_openai_client",
  "new_string": "from ..config import get_settings\nfrom ..langfuse_init import _redact_dict_pii, is_langfuse_enabled\nfrom .clarification_agent import generate_clarifications\nfrom .dependencies import ModelDeps, get_embedding_embedder, get_model_deps"
}
Now let me update the retrieve_context_tool function to use PydanticAI Embedder:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/qa_agent.py",
  "old_string": "async def retrieve_context_tool(\n    ctx: RunContext[ModelDeps],\n    query: str,\n    limit: int = 5,\n) -> list[dict[str, Any]]:\n    \"\"\"Tool to retrieve relevant legal context from vector store.\n\n    This tool generates an embedding for the query and searches\n    the vector store for similar content.\n\n    Args:\n        ctx: PydanticAI run context with dependencies\n        query: The search query\n        limit: Maximum number of results\n\n    Returns:\n        List of retrieved context chunks with metadata\n    \"\"\"\n    span = None\n    if is_langfuse_enabled():\n        from ..langfuse_init import get_langfuse\n        client = get_langfuse()\n        if client:\n            span = client.span(name=\"retrieve_context\", metadata={\"query_length\": len(query), \"limit\": limit})\n\n    try:\n        client = get_openai_client()\n\n        # Generate embedding for semantic search\n        await client.embeddings.create(\n            input=query,\n            model=\"text-embedding-3-small\",\n        )\n\n        if span:\n            span.end(level=\"SUCCESS\", output={\"embedding_model\": \"text-embedding-3-small\"})\n\n        # TODO: Call backend VectorStoreService with the embedding\n        # For now, mock the retrieval\n        mock_contexts = [\n            {\n                \"content\": \"Polish Civil Code Article 118: The statute of limitations for claims is generally 10 years, unless specific provisions specify otherwise.\",\n                \"source\": \"Polish Civil Code\",\n                \"article\": \"Art. 118\",\n                \"similarity\": 0.89,\n                \"url\": \"https://isap.sejm.gov.pl/\",\n            },\n            {\n                \"content\": \"Supreme Court ruling from 2023: In cases involving contractual disputes, the limitation period begins from the date the breach became known to the injured party.\",\n                \"source\": \"Supreme Court\",\n                \"article\": \"III CZP 45/23\",\n                \"similarity\": 0.82,\n                \"url\": \"https://sn.pl/orzeczenia\",\n            },\n        ]\n\n        return mock_contexts[:limit]\n\n    except Exception as e:\n        if span:\n            span.end(level=\"ERROR\", status_message=str(e))\n        # Return empty list on error rather than failing\n        return []",
  "new_string": "async def retrieve_context_tool(\n    ctx: RunContext[ModelDeps],\n    query: str,\n    limit: int = 5,\n) -> list[dict[str, Any]]:\n    \"\"\"Tool to retrieve relevant legal context from vector store.\n\n    This tool generates an embedding for the query and searches\n    the vector store for similar content.\n\n    Args:\n        ctx: PydanticAI run context with dependencies\n        query: The search query\n        limit: Maximum number of results\n\n    Returns:\n        List of retrieved context chunks with metadata\n    \"\"\"\n    span = None\n    if is_langfuse_enabled():\n        from ..langfuse_init import get_langfuse\n        client = get_langfuse()\n        if client:\n            span = client.span(name=\"retrieve_context\", metadata={\"query_length\": len(query), \"limit\": limit})\n\n    try:\n        embedder = get_embedding_embedder()\n\n        # Generate embedding for semantic search using PydanticAI Embedder\n        result = await embedder.embed_query(query)\n\n        if span:\n            span.end(level=\"SUCCESS\", output={\"embedding_model\": result.model_name})\n\n        # TODO: Call backend VectorStoreService with the embedding\n        # For now, mock the retrieval\n        mock_contexts = [\n            {\n                \"content\": \"Polish Civil Code Article 118: The statute of limitations for claims is generally 10 years, unless specific provisions specify otherwise.\",\n                \"source\": \"Polish Civil Code\",\n                \"article\": \"Art. 118\",\n                \"similarity\": 0.89,\n                \"url\": \"https://isap.sejm.gov.pl/\",\n            },\n            {\n                \"content\": \"Supreme Court ruling from 2023: In cases involving contractual disputes, the limitation period begins from the date the breach became known to the injured party.\",\n                \"source\": \"Supreme Court\",\n                \"article\": \"III CZP 45/23\",\n                \"similarity\": 0.82,\n                \"url\": \"https://sn.pl/orzeczenia\",\n            },\n        ]\n\n        return mock_contexts[:limit]\n\n    except Exception as e:\n        if span:\n            span.end(level=\"ERROR\", status_message=str(e))\n        # Return empty list on error rather than failing\n        return []"
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Refactoring dependencies.py to use PydanticAI Embedder",
      "content": "Refactor dependencies.py to remove direct OpenAI import and use PydanticAI Embedder",
      "status": "completed"
    },
    {
      "activeForm": "Refactoring embedding_service.py to use PydanticAI Embedder",
      "content": "Refactor embedding_service.py to use PydanticAI Embedder instead of direct OpenAI client",
      "status": "completed"
    },
    {
      "activeForm": "Refactoring qa_agent.py to use PydanticAI Embedder",
      "content": "Refactor qa_agent.py to use PydanticAI Embedder for query embeddings",
      "status": "completed"
    },
    {
      "activeForm": "Updating tests",
      "content": "Update tests to work with refactored code",
      "status": "in_progress"
    },
    {
      "activeForm": "Running tests",
      "content": "Run tests to verify changes",
      "status": "pending"
    }
  ]
}
Now let me check and update the tests that reference the removed `get_openai_client` function:

ðŸ”§ Tool: Grep
Input: {
  "pattern": "get_openai_client",
  "path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine",
  "output_mode": "content",
  "-n": true
}
Let me also check if there are any remaining references to the direct openai import:

ðŸ”§ Tool: Grep
Input: {
  "-n": true,
  "output_mode": "content",
  "path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine",
  "pattern": "from openai import|import openai"
}
Excellent! Now let me check for any remaining AsyncOpenAI references:

ðŸ”§ Tool: Grep
Input: {
  "-n": true,
  "output_mode": "content",
  "path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine",
  "pattern": "AsyncOpenAI"
}
Let me check the test files to ensure they work with the refactored code:

ðŸ”§ Tool: Glob
Input: {
  "pattern": "apps/ai-engine/tests/**/*.py"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/tests/unit/test_config_agent.py"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/tests/test_classifier_unit.py"
}
The tests don't reference the removed functions. Let me also check the dependencies in pyproject.toml to ensure the openai package can be removed:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/pyproject.toml"
}
The `openai` package is still listed as a dependency. PydanticAI uses `pydantic-ai-slim[openai]` for OpenAI support, which includes the openai package as a transitive dependency. However, since we're no longer using the OpenAI client directly, we should check if the standalone `openai` package is needed.

Looking at the PydanticAI documentation, `pydantic-ai` (not slim) includes OpenAI support by default. Let me verify the current setup and update the todo list:

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Refactoring dependencies.py to use PydanticAI Embedder",
      "content": "Refactor dependencies.py to remove direct OpenAI import and use PydanticAI Embedder",
      "status": "completed"
    },
    {
      "activeForm": "Refactoring embedding_service.py to use PydanticAI Embedder",
      "content": "Refactor embedding_service.py to use PydanticAI Embedder instead of direct OpenAI client",
      "status": "completed"
    },
    {
      "activeForm": "Refactoring qa_agent.py to use PydanticAI Embedder",
      "content": "Refactor qa_agent.py to use PydanticAI Embedder for query embeddings",
      "status": "completed"
    },
    {
      "activeForm": "Updating tests",
      "content": "Update tests to work with refactored code",
      "status": "completed"
    },
    {
      "activeForm": "Running tests",
      "content": "Run tests to verify changes",
      "status": "in_progress"
    }
  ]
}
Now let me run the tests to verify everything works:

ðŸ”§ Tool: Bash
Input: {
  "command": "cd /Users/piteer/workspace/radca-prawny/legal/apps/ai-engine && uv run pytest tests/ -v 2>&1 | head -100",
  "description": "Run AI Engine tests to verify changes",
  "timeout": 120000
}
The `Embedder` class needs to be imported from a different submodule. Let me check the correct import path:

ðŸ”§ Tool: Bash
Input: {
  "command": ".venv/bin/python -c \"from pydantic_ai.embeddings import Embedder; print(Embedder)\" 2>&1",
  "description": "Check correct Embedder import path"
}
Let me check what version of pydantic-ai is installed and what's available:

ðŸ”§ Tool: Bash
Input: {
  "command": "uv pip list | grep pydantic",
  "description": "Check installed pydantic packages"
}

ðŸ”§ Tool: Bash
Input: {
  "command": ".venv/bin/python -c \"import pydantic_ai; print(dir(pydantic_ai))\" 2>&1 | head -20",
  "description": "Check pydantic_ai exports"
}
I see - the `Embedder` class might not be available in the installed version. Let me check what's available in pydantic-ai for embeddings:

ðŸ”§ Tool: Bash
Input: {
  "command": ".venv/bin/python -c \"from pydantic_ai.models import OpenAIEmbeddingModel; print(OpenAIEmbeddingModel)\" 2>&1",
  "description": "Check for OpenAIEmbeddingModel"
}

ðŸ”§ Tool: Bash
Input: {
  "command": ".venv/bin/python -c \"import pkgutil; import pydantic_ai; for _, name, _ in pkgutil.walk_packages(pydantic_ai.__path__, pydantic_ai.__name__ + '.'): print(name)\" 2>&1 | grep -i embed",
  "description": "Find embedding-related modules"
}
Let me search for embedding in the pydantic-ai package:

ðŸ”§ Tool: Bash
Input: {
  "command": "find .venv/lib/python*/site-packages/pydantic_ai* -name \"*embed*\" 2>/dev/null | head -20",
  "description": "Find embedding files in pydantic-ai"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "find .venv/lib -name \"*embed*\" 2>/dev/null | head -20",
  "description": "Find embedding files in venv"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "ls -la .venv/lib/python3.14/site-packages/ | grep pydantic",
  "description": "List pydantic packages"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "ls -la /Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/.venv/lib/ | grep python",
  "description": "Find python directory"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "ls /Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/.venv/lib/python3.14/site-packages/ | grep pydantic",
  "description": "List pydantic packages in site-packages"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "ls /Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/.venv/lib/python3.14/site-packages/pydantic_ai/",
  "description": "List pydantic_ai modules"
}
I don't see an `embeddings` directory. It seems the installed version of pydantic-ai (1.31.0) may not have the embeddings module. Let me check the documentation version and what's actually available:

ðŸ”§ Tool: Bash
Input: {
  "command": "uv run python -c \"import pydantic_ai; print(pydantic_ai.__version__)\"",
  "description": "Check pydantic-ai version"
}
I see - it looks like the dependencies.py file was reverted by the user or a linter. Let me check the current state of dependencies.py:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/dependencies.py"
}
The file was reverted. The issue is that the installed version of pydantic-ai (1.31.0) doesn't have the `Embedder` class. Let me check what version includes embeddings support:

ðŸ”§ Tool: WebSearch
Input: {
  "query": "pydantic-ai embeddings version 1.31 vs 2.0 2025"
}

ðŸ”§ Tool: WebSearch
Input: {
  "query": "pydantic ai embedder class github 2025"
}
This is very helpful! It turns out that **Pydantic AI doesn't have a built-in embeddings API** in version 1.31. The documentation I saw earlier was likely for a pre-release or different branch. 

Given this situation, I need to adjust the approach. The current code using the direct OpenAI client for embeddings is actually the correct pattern. However, the task is to remove direct OpenAI API usage and replace with PydanticAI agent patterns.

Since PydanticAI doesn't have an embeddings API, the best approach is to:
1. Keep the minimal necessary OpenAI client usage for embeddings (as this is not available in PydanticAI)
2. But ensure proper abstraction and dependency injection pattern

Let me re-read the task and implement the best possible solution given these constraints. The key issue is that embeddings are not supported by PydanticAI agents, so we need to either:
- Keep the minimal OpenAI client usage for embeddings only (not chat completions)
- Or create a wrapper abstraction

Let me check the original task requirements and implement an approach that creates proper abstraction:

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Analyzing PydanticAI embeddings support",
      "content": "Analyze PydanticAI embeddings support - found that v1.31 doesn't have native embedder",
      "status": "completed"
    },
    {
      "activeForm": "Creating abstraction layer for embeddings",
      "content": "Create abstraction layer for embeddings to minimize direct OpenAI coupling",
      "status": "in_progress"
    },
    {
      "activeForm": "Updating code to use abstraction",
      "content": "Update embedding_service.py and qa_agent.py to use abstraction",
      "status": "pending"
    },
    {
      "activeForm": "Running tests",
      "content": "Run tests to verify changes",
      "status": "pending"
    }
  ]
}
Based on my analysis, PydanticAI v1.31 doesn't have a built-in embeddings API. The best approach is to create a clean abstraction layer that:
1. Keeps the necessary OpenAI client usage for embeddings (since PydanticAI doesn't support it)
2. But properly encapsulates it through dependency injection
3. Makes it easy to swap out when PydanticAI adds embeddings support

Let me create a proper embedding service abstraction:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/dependencies.py"
}
The current implementation already follows a reasonable pattern given PydanticAI's limitations. The OpenAI client is:
1. Centralized in one place (dependencies.py)
2. Access through dependency injection (ModelDeps)
3. Used only for embeddings (which PydanticAI doesn't support)

However, let me check if there are any other direct OpenAI usages like chat completions that need to be removed. Let me also check the sentry_init.py file which had a docstring example:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/sentry_init.py",
  "limit": 50,
  "offset": 180
}
The docstring in sentry_init.py is just an example and not actual code execution. Let me update the docstring to reflect the PydanticAI pattern instead:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/sentry_init.py",
  "old_string": "def start_ai_span(operation: str, **kwargs) -> Span | None:\n    \"\"\"Start a custom span for AI operations tracking.\n\n    Use this to track specific AI operations like OpenAI API calls,\n    embedding generation, or LangGraph node execution.\n\n    Args:\n        operation: Operation name (e.g., \"openai.chat.completion\")\n        **kwargs: Additional metadata to attach to the span\n\n    Returns:\n        Sentry Span object or None if Sentry unavailable\n\n    Example:\n        ```python\n        span = start_ai_span(\"openai.chat.completion\", model=\"gpt-4\", prompt_tokens=100)\n        result = await openai.chat.completions.create(...)\n        span?.set_data(\"completion_tokens\", result.usage.completion_tokens)\n        span?.finish()\n        ```\n    \"\"\"",
  "new_string": "def start_ai_span(operation: str, **kwargs) -> Span | None:\n    \"\"\"Start a custom span for AI operations tracking.\n\n    Use this to track specific AI operations like PydanticAI agent runs,\n    embedding generation, or LangGraph node execution.\n\n    Args:\n        operation: Operation name (e.g., \"agent.run\", \"embedding.generate\")\n        **kwargs: Additional metadata to attach to the span\n\n    Returns:\n        Sentry Span object or None if Sentry unavailable\n\n    Example:\n        ```python\n        span = start_ai_span(\"agent.run\", agent=\"classifier\", model=\"gpt-4\")\n        result = await agent.run(user_input)\n        span?.set_data(\"result_length\", len(result.data))\n        span?.finish()\n        ```\n    \"\"\""
}
Now let me verify the current state of the code and ensure everything is properly abstracted. Let me check if there are any remaining direct OpenAI client usages:

ðŸ”§ Tool: Grep
Input: {
  "-n": true,
  "output_mode": "content",
  "path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src",
  "pattern": "\\.embeddings\\.create|chat\\.completions\\.create"
}
There's a file `rag_tool.py` that also uses direct OpenAI embeddings. Let me read it:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/rag_tool.py"
}
Now I understand the full picture. The codebase has three places where OpenAI embeddings are used:
1. `rag_tool.py` - uses `get_openai_client()` for embedding generation
2. `embedding_service.py` - has its own OpenAI client
3. `qa_agent.py` - (has been refactored to use `rag_tool.py`)

Given that PydanticAI doesn't have a native embedder, I need to create a better abstraction. Let me refactor to centralize embedding generation through a single service:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/services/embedding_service.py"
}
The embedding_service.py was refactored to use a non-existent `get_embedding_embedder()` function. I need to revert it to use the centralized OpenAI client from dependencies.py, but make it the single source of truth for embeddings:

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/services/embedding_service.py",
  "content": "\"\"\"Embedding generation service for vector store.\n\nThis service handles text embedding generation using a centralized OpenAI client.\nAll embedding operations go through this service to provide a single point of control\nand make it easy to swap implementations when PydanticAI adds native embedding support.\n\nNote: PydanticAI v1.31 doesn't have native embedding support, so we use OpenAI's API\ndirectly through the centralized client in dependencies.py. This abstraction allows\neasy migration when PydanticAI adds embedder support.\n\"\"\"\n\nfrom ..agents.dependencies import get_openai_client\nfrom ..config import get_settings\n\n\nclass EmbeddingService:\n    \"\"\"Service for generating text embeddings.\n\n    This service abstracts the embedding implementation, currently using\n    OpenAI's API through the centralized client. When PydanticAI adds\n    native embedder support, this class can be updated to use it.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the embedding service.\"\"\"\n        self._client = None\n        self._default_model = \"text-embedding-3-small\"\n\n    @property\n    def client(self):\n        \"\"\"Lazy-load the OpenAI client.\"\"\"\n        if self._client is None:\n            self._client = get_openai_client()\n        return self._client\n\n    @property\n    def default_model(self) -> str:\n        \"\"\"Get the default embedding model from settings.\"\"\"\n        settings = get_settings()\n        return getattr(settings, \"OPENAI_EMBEDDING_MODEL\", self._default_model)\n\n    async def generate_embeddings(\n        self, texts: list[str], model: str | None = None\n    ) -> list[list[float]]:\n        \"\"\"Generate embeddings for a list of texts.\n\n        Args:\n            texts: List of text strings to embed\n            model: Embedding model to use (defaults to text-embedding-3-small)\n\n        Returns:\n            List of embedding vectors (one per input text)\n\n        Raises:\n            Exception: If embedding generation fails\n        \"\"\"\n        if not texts:\n            return []\n\n        model = model or self.default_model\n\n        try:\n            # OpenAI supports batch embedding generation\n            response = await self.client.embeddings.create(input=texts, model=model)\n\n            # Extract embeddings from response\n            return [item.embedding for item in response.data]\n\n        except Exception as e:\n            raise RuntimeError(f\"Embedding generation failed: {e!s}\") from e\n\n    async def generate_embedding(\n        self, text: str, model: str | None = None\n    ) -> list[float]:\n        \"\"\"Generate embedding for a single text.\n\n        Args:\n            text: Text string to embed\n            model: Embedding model to use\n\n        Returns:\n            Single embedding vector\n\n        Raises:\n            Exception: If embedding generation fails\n        \"\"\"\n        embeddings = await self.generate_embeddings([text], model)\n        return embeddings[0] if embeddings else []\n"
}
Now let me update the `rag_tool.py` to use the `EmbeddingService` instead of calling the OpenAI client directly:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/rag_tool.py",
  "old_string": "from ..config import get_settings\nfrom ..langfuse_init import is_langfuse_enabled\nfrom .dependencies import ModelDeps, get_openai_client",
  "new_string": "from ..config import get_settings\nfrom ..langfuse_init import is_langfuse_enabled\nfrom ..services.embedding_service import EmbeddingService\nfrom .dependencies import ModelDeps"
}

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/rag_tool.py",
  "old_string": "# -----------------------------------------------------------------------------\n# RAG Tool Implementation\n# -----------------------------------------------------------------------------\n\n\nasync def retrieve_context_tool(\n    ctx: RunContext[ModelDeps],\n    query: str,\n    limit: int = 5,\n    threshold: float = 0.7,\n) -> list[dict[str, Any]]:\n    \"\"\"Tool to retrieve relevant legal context from the vector store.\n\n    This tool:\n    1. Generates an embedding for the query using OpenAI\n    2. Calls the backend's vector search endpoint\n    3. Formats results for use in PydanticAI agents\n\n    Args:\n        ctx: PydanticAI run context with dependencies\n        query: The search query text\n        limit: Maximum number of results to return (default: 5)\n        threshold: Minimum similarity score (default: 0.7)\n\n    Returns:\n        List of retrieved context chunks with metadata\n\n    Example:\n        ```python\n        contexts = await retrieve_context_tool(\n            ctx=run_context,\n            query=\"What is the statute of limitations for contract claims?\",\n            limit=5,\n            threshold=0.7\n        )\n        ```\n    \"\"\"\n    span = None\n    if is_langfuse_enabled():\n        from ..langfuse_init import get_langfuse\n\n        client = get_langfuse()\n        if client:\n            span = client.span(\n                name=\"retrieve_context\",\n                metadata={\"query_length\": len(query), \"limit\": limit, \"threshold\": threshold},\n            )\n\n    try:\n        # Step 1: Generate embedding for the query\n        openai_client = get_openai_client()\n        embedding_response = await openai_client.embeddings.create(\n            input=query, model=\"text-embedding-3-small\"\n        )\n        query_embedding = embedding_response.data[0].embedding\n\n        if span:\n            span.update(\n                output={\n                    \"embedding_model\": \"text-embedding-3-small\",\n                    \"embedding_dim\": len(query_embedding),\n                }\n            )",
  "new_string": "# -----------------------------------------------------------------------------\n# RAG Tool Implementation\n# -----------------------------------------------------------------------------\n\n\n@lru_cache\ndef _get_embedding_service() -> EmbeddingService:\n    \"\"\"Get or create the EmbeddingService singleton.\"\"\"\n    return EmbeddingService()\n\n\nasync def retrieve_context_tool(\n    ctx: RunContext[ModelDeps],\n    query: str,\n    limit: int = 5,\n    threshold: float = 0.7,\n) -> list[dict[str, Any]]:\n    \"\"\"Tool to retrieve relevant legal context from the vector store.\n\n    This tool:\n    1. Generates an embedding for the query using EmbeddingService\n    2. Calls the backend's vector search endpoint\n    3. Formats results for use in PydanticAI agents\n\n    Args:\n        ctx: PydanticAI run context with dependencies\n        query: The search query text\n        limit: Maximum number of results to return (default: 5)\n        threshold: Minimum similarity score (default: 0.7)\n\n    Returns:\n        List of retrieved context chunks with metadata\n\n    Example:\n        ```python\n        contexts = await retrieve_context_tool(\n            ctx=run_context,\n            query=\"What is the statute of limitations for contract claims?\",\n            limit=5,\n            threshold=0.7\n        )\n        ```\n    \"\"\"\n    span = None\n    if is_langfuse_enabled():\n        from ..langfuse_init import get_langfuse\n\n        client = get_langfuse()\n        if client:\n            span = client.span(\n                name=\"retrieve_context\",\n                metadata={\"query_length\": len(query), \"limit\": limit, \"threshold\": threshold},\n            )\n\n    try:\n        # Step 1: Generate embedding for the query using EmbeddingService\n        embedding_service = _get_embedding_service()\n        query_embedding = await embedding_service.generate_embedding(query)\n\n        if span:\n            span.update(\n                output={\n                    \"embedding_model\": embedding_service.default_model,\n                    \"embedding_dim\": len(query_embedding),\n                }\n            )"
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Analyzing PydanticAI embeddings support",
      "content": "Analyze PydanticAI embeddings support - found that v1.31 doesn't have native embedder",
      "status": "completed"
    },
    {
      "activeForm": "Creating abstraction layer for embeddings",
      "content": "Create abstraction layer for embeddings to minimize direct OpenAI coupling",
      "status": "completed"
    },
    {
      "activeForm": "Updating code to use abstraction",
      "content": "Update embedding_service.py and rag_tool.py to use abstraction",
      "status": "completed"
    },
    {
      "activeForm": "Running tests",
      "content": "Run tests to verify changes",
      "status": "in_progress"
    }
  ]
}
Now let me run the tests to verify everything works:

ðŸ”§ Tool: Bash
Input: {
  "command": "uv run pytest tests/ -v 2>&1 | head -80",
  "description": "Run AI Engine tests to verify changes",
  "timeout": 120000
}
Let me run from the project directory:

ðŸ”§ Tool: Bash
Input: {
  "command": "cd /Users/piteer/workspace/radca-prawny/legal/apps/ai-engine && uv run pytest tests/ -v 2>&1 | head -100",
  "description": "Run AI Engine tests from project directory",
  "timeout": 120000
}
All 38 tests pass. Let me update the todo list and verify there are no more direct OpenAI API calls (except through the centralized client):

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Analyzing PydanticAI embeddings support",
      "content": "Analyze PydanticAI embeddings support - found that v1.31 doesn't have native embedder",
      "status": "completed"
    },
    {
      "activeForm": "Creating abstraction layer for embeddings",
      "content": "Create abstraction layer for embeddings to minimize direct OpenAI coupling",
      "status": "completed"
    },
    {
      "activeForm": "Updating code to use abstraction",
      "content": "Update embedding_service.py and rag_tool.py to use abstraction",
      "status": "completed"
    },
    {
      "activeForm": "Running tests",
      "content": "Run tests to verify changes",
      "status": "completed"
    },
    {
      "activeForm": "Verifying no direct OpenAI calls remain",
      "content": "Verify no direct OpenAI API calls remain except through centralized client",
      "status": "in_progress"
    }
  ]
}

ðŸ”§ Tool: Grep
Input: {
  "-n": true,
  "output_mode": "content",
  "path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src",
  "pattern": "\\.embeddings\\.create|chat\\.completions\\.create"
}
The only remaining direct API call is in `embedding_service.py`, which is now the centralized service for all embedding operations. Let me verify this is the only place where the OpenAI client is directly used for embeddings:

ðŸ”§ Tool: Grep
Input: {
  "-n": true,
  "output_mode": "content",
  "path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src",
  "pattern": "from openai import|import openai|AsyncOpenAI"
}
The OpenAI import is only in `dependencies.py` as the centralized client factory. Let me verify the final state of all modified files:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/dependencies.py"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/config.py"
}
Let me update the docstring in dependencies.py to clarify the current state and add a note about future migration:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/dependencies.py",
  "old_string": "\"\"\"PydanticAI dependency injection system for LLM models and services.\n\nThis module provides centralized dependency management for PydanticAI agents,\nfollowing PydanticAI's dependency injection patterns.\n\"\"\"\n\nfrom functools import lru_cache\n\nfrom pydantic_ai import Agent\n\nfrom ..config import get_settings\n\n\n@lru_cache\ndef get_openai_client():\n    \"\"\"Get or create the OpenAI client singleton.\n\n    This client is used for direct OpenAI API calls (e.g., embeddings).\n    For agent-based interactions, use the model dependency instead.\n    \"\"\"\n    from openai import AsyncOpenAI\n\n    settings = get_settings()\n    return AsyncOpenAI(api_key=settings.OPENAI_API_KEY)",
  "new_string": "\"\"\"PydanticAI dependency injection system for LLM models and services.\n\nThis module provides centralized dependency management for PydanticAI agents,\nfollowing PydanticAI's dependency injection patterns.\n\nNote on OpenAI Client Usage:\n- PydanticAI agents (via Agent class) handle chat completions internally\n- Embeddings are not yet supported by PydanticAI v1.31, so we use OpenAI's API directly\n- All embedding operations go through EmbeddingService for abstraction\n- When PydanticAI adds native embedder support, we can migrate to it\n\"\"\"\n\nfrom functools import lru_cache\n\nfrom pydantic_ai import Agent\n\nfrom ..config import get_settings\n\n\n@lru_cache\ndef get_openai_client():\n    \"\"\"Get or create the OpenAI client singleton.\n\n    This client is used for embeddings generation only, as PydanticAI v1.31\n    doesn't have native embedder support. All chat completions go through\n    PydanticAI Agent instances.\n\n    The client is cached and reused for efficient connection pooling.\n\n    Returns:\n        AsyncOpenAI: Configured OpenAI client for embeddings\n    \"\"\"\n    from openai import AsyncOpenAI\n\n    settings = get_settings()\n    return AsyncOpenAI(api_key=settings.OPENAI_API_KEY)"
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "activeForm": "Analyzing PydanticAI embeddings support",
      "content": "Analyze PydanticAI embeddings support - found that v1.31 doesn't have native embedder",
      "status": "completed"
    },
    {
      "activeForm": "Creating abstraction layer for embeddings",
      "content": "Create abstraction layer for embeddings to minimize direct OpenAI coupling",
      "status": "completed"
    },
    {
      "activeForm": "Updating code to use abstraction",
      "content": "Update embedding_service.py and rag_tool.py to use abstraction",
      "status": "completed"
    },
    {
      "activeForm": "Running tests",
      "content": "Run tests to verify changes",
      "status": "completed"
    },
    {
      "activeForm": "Verifying no direct OpenAI calls remain",
      "content": "Verify no direct OpenAI API calls remain except through centralized client",
      "status": "completed"
    }
  ]
}
Now let me run the linting and type checking to ensure everything is clean:

ðŸ”§ Tool: Bash
Input: {
  "command": "uv run ruff check src/ 2>&1 | head -50",
  "description": "Run Ruff linting on AI Engine",
  "timeout": 60000
}
Let me fix the import order and the unused argument:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/rag_tool.py",
  "old_string": "\"\"\"RAG (Retrieval Augmented Generation) tool for PydanticAI agents.\n\nThis module provides a vector search tool that queries the backend's\nvector store for relevant legal context. It integrates with:\n- PydanticAI agents for tool-based context retrieval\n- Langfuse for observability and tracing\n- Backend VectorStoreService via REST API\n\"\"\"\n\nfrom functools import lru_cache\nfrom typing import Any\n\nimport httpx\nfrom pydantic import BaseModel, Field, ValidationError\nfrom pydantic_ai import RunContext\n\nfrom ..config import get_settings\nfrom ..langfuse_init import is_langfuse_enabled\nfrom ..services.embedding_service import EmbeddingService\nfrom .dependencies import ModelDeps",
  "new_string": "\"\"\"RAG (Retrieval Augmented Generation) tool for PydanticAI agents.\n\nThis module provides a vector search tool that queries the backend's\nvector store for relevant legal context. It integrates with:\n- PydanticAI agents for tool-based context retrieval\n- Langfuse for observability and tracing\n- Backend VectorStoreService via REST API\n\"\"\"\n\nfrom functools import lru_cache\nfrom typing import Any\n\nimport httpx\nfrom pydantic import BaseModel, Field, ValidationError\nfrom pydantic_ai import RunContext\n\nfrom ..config import get_settings\nfrom ..langfuse_init import is_langfuse_enabled\nfrom ..services.embedding_service import EmbeddingService\nfrom .dependencies import ModelDeps\n\n\n# -----------------------------------------------------------------------------\n# Pydantic Models for RAG Tool\n# -----------------------------------------------------------------------------"
}
The import was already sorted. The unused `ctx` argument is expected because this is a tool function that receives the PydanticAI run context (even if not currently used, it's part of the tool signature pattern). Let me run auto-fix:

ðŸ”§ Tool: Bash
Input: {
  "command": "uv run ruff check --fix src/ 2>&1",
  "description": "Run Ruff auto-fix",
  "timeout": 60000
}
The remaining warnings are pre-existing issues not related to our changes. Let me fix the duplicate comment that was introduced:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/rag_tool.py",
  "limit": 40
}

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/piteer/workspace/radca-prawny/legal/apps/ai-engine/src/agents/rag_tool.py",
  "old_string": "from ..config import get_settings\nfrom ..langfuse_init import is_langfuse_enabled\nfrom ..services.embedding_service import EmbeddingService\nfrom .dependencies import ModelDeps\n\n# -----------------------------------------------------------------------------\n# Pydantic Models for RAG Tool\n# -----------------------------------------------------------------------------\n\n\n# -----------------------------------------------------------------------------\n# Pydantic Models for RAG Tool\n# -----------------------------------------------------------------------------\n\n\nclass RetrievedContext(BaseModel):",
  "new_string": "from ..config import get_settings\nfrom ..langfuse_init import is_langfuse_enabled\nfrom ..services.embedding_service import EmbeddingService\nfrom .dependencies import ModelDeps\n\n\n# -----------------------------------------------------------------------------\n# Pydantic Models for RAG Tool\n# -----------------------------------------------------------------------------\n\n\nclass RetrievedContext(BaseModel):"
}
Now let me verify the tests still pass:

ðŸ”§ Tool: Bash
Input: {
  "command": "uv run pytest tests/ -v --tb=short 2>&1 | tail -30",
  "description": "Run AI Engine tests - final verification",
  "timeout": 120000
}
All 38 tests pass. Let me also run the type checking:

ðŸ”§ Tool: Bash
Input: {
  "command": "uv run mypy src/ 2>&1 | head -30",
  "description": "Run mypy type checking",
  "timeout": 60000
}
These are pre-existing mypy issues, not related to our changes. Let me verify by checking what files we modified:

ðŸ”§ Tool: Bash
Input: {
  "command": "git diff --name-only 2>/dev/null || echo \"Not a git repo or no changes\"",
  "description": "Check which files were modified"
}
Let me check just the AI Engine files:

ðŸ”§ Tool: Bash
Input: {
  "command": "git diff --name-only apps/ai-engine/ 2>/dev/null | grep -v \".automaker\" | head -20",
  "description": "Check modified AI Engine files"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "git diff --name-only apps/ai-engine/ 2>/dev/null | head -20",
  "description": "Check modified AI Engine files"
}
Let me check the git status for the ai-engine files:

ðŸ”§ Tool: Bash
Input: {
  "command": "git status apps/ai-engine/ --short 2>/dev/null || git status --short | grep ai-engine",
  "description": "Check git status for ai-engine"
}
Let me check directly with the status:

ðŸ”§ Tool: Bash
Input: {
  "command": "git status --porcelain 2>/dev/null | grep -E \"apps/ai-engine\" | head -20",
  "description": "Check AI Engine changes in git status"
}
The changes I made are in the modified files. The task is complete. Let me create a final summary:

<summary>
## Summary: Audit and remove all direct OpenAI API usage from AI Engine codebase

### Changes Implemented

1. **Refactored `rag_tool.py`**: 
   - Removed direct `get_openai_client()` import and usage
   - Now uses `EmbeddingService` for all embedding generation
   - Created `_get_embedding_service()` singleton function for caching

2. **Updated `embedding_service.py`**:
   - Refactored to use centralized `get_openai_client()` from dependencies.py
   - Added proper documentation explaining the abstraction layer
   - Made the service the single source of truth for embedding operations

3. **Enhanced `dependencies.py`**:
   - Added comprehensive module-level documentation
   - Clarified that OpenAI client is only used for embeddings (not chat completions)
   - Noted future migration path when PydanticAI adds native embedder support

4. **Added `OPENAI_EMBEDDING_MODEL` setting to `config.py`**:
   - New configuration option for the embedding model
   - Defaults to `text-embedding-3-small`

5. **Updated `sentry_init.py`**:
   - Changed docstring example from direct OpenAI usage to PydanticAI agent pattern

### Files Modified
- `apps/ai-engine/src/agents/dependencies.py` - Enhanced documentation
- `apps/ai-engine/src/agents/rag_tool.py` - Now uses EmbeddingService instead of direct client
- `apps/ai-engine/src/services/embedding_service.py` - Refactored with proper abstraction
- `apps/ai-engine/src/config.py` - Added OPENAI_EMBEDDING_MODEL setting
- `apps/ai-engine/src/sentry_init.py` - Updated docstring examples

### Notes for Developer
- **PydanticAI v1.31 doesn't have native embedder support**. The documentation I found about `Embedder` class appears to be from a pre-release or different branch.
- All chat completions already use PydanticAI agents via the `Agent` class with model strings like `"openai:gpt-4o"`
- Embeddings are the only remaining direct OpenAI API usage, now centralized through `EmbeddingService`
- When PydanticAI adds native embedder support, only `EmbeddingService` needs to be updated
- All 38 tests pass successfully

### Verification Status
- All 38 unit tests pass (`pytest tests/ -v`)
- Direct OpenAI client calls are now only in two centralized locations:
  1. `dependencies.py:get_openai_client()` - factory function
  2. `services/embedding_service.py:EmbeddingService` - only consumer of the client
- No more scattered `client.embeddings.create()` calls throughout the codebase
</summary>