{
  "category": "bug",
  "description": "Fix the streaming endpoint to send tokens immediately as they're generated by the LLM. The issue is likely in how the async generator yields data or how the StreamingResponse is configured. Ensure: 1) The async generator yields tokens immediately without buffering, 2) No JSON accumulation is happening before yielding, 3) The response is not being buffered by FastAPI middleware (disable buffer for streaming routes), 4) The SSE events are sent with proper newlines, 5) The client-side can consume tokens as they arrive. May need to use yield instead of await, or configure the response to flush buffers immediately.",
  "id": "fix-streaming-buffering-issue",
  "title": "Fix Streaming Response Buffering Issue",
  "dependencies": [
    "verify-streaming-response-works-immediately"
  ],
  "priority": 1,
  "status": "backlog",
  "branchName": "streaming-response",
  "descriptionHistory": [
    {
      "description": "Fix the streaming endpoint to send tokens immediately as they're generated by the LLM. The issue is likely in how the async generator yields data or how the StreamingResponse is configured. Ensure: 1) The async generator yields tokens immediately without buffering, 2) No JSON accumulation is happening before yielding, 3) The response is not being buffered by FastAPI middleware (disable buffer for streaming routes), 4) The SSE events are sent with proper newlines, 5) The client-side can consume tokens as they arrive. May need to use yield instead of await, or configure the response to flush buffers immediately.",
      "timestamp": "2026-01-27T12:55:09.393Z",
      "source": "initial"
    }
  ]
}